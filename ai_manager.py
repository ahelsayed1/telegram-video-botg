# ai_manager.py - Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
import os
import logging
import asyncio
import google.generativeai as genai
import openai
import requests
import aiohttp
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Tuple
import base64
import json

logger = logging.getLogger(__name__)

class AIManager:
    """Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„"""
    
    def __init__(self, db):
        self.db = db
        self.setup_apis()
        self.user_limits_cache = {}  # ÙƒØ§Ø´ Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        self.conversation_history = {}  # ÙƒØ§Ø´ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        self.max_history_length = 10  # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
    def setup_apis(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Google Gemini
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                logger.info("âœ… Google Gemini API configured successfully")
            else:
                self.gemini_available = False
                logger.warning("âš ï¸ Google Gemini API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
                logger.info("âœ… OpenAI API configured successfully")
            else:
                self.openai_available = False
                logger.warning("âš ï¸ OpenAI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Luma AI
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            if self.luma_available:
                logger.info("âœ… Luma AI API configured successfully")
            else:
                logger.warning("âš ï¸ Luma AI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup AI APIs: {e}")
            self.gemini_available = False
            self.openai_available = False
            self.luma_available = False
    
    # ==================== Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ ====================
    
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                # Ø¬Ù„Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                    SELECT usage_count FROM ai_usage 
                    WHERE user_id = ? AND service_type = ? AND usage_date = ?
                    ''', (user_id, service_type, today))
                    
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø©
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            remaining = limit - current_usage
            
            if current_usage >= limit:
                return False, 0  # ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯
            return True, remaining  # Ù…Ø§ Ø²Ø§Ù„ Ù…ØªØ¨Ù‚ÙŠ
            
        except Exception as e:
            logger.error(f"âŒ Error checking user limit: {e}")
            return True, 999  # Ø§Ù„Ø³Ù…Ø§Ø­ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø´
            current_usage = self.user_limits_cache.get(cache_key, 0)
            self.user_limits_cache[cache_key] = current_usage + 1
            
            # ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                
                conn.commit()
                logger.debug(f"âœ… Updated usage for user {user_id}, service {service_type}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Error updating user usage: {e}")
            return False
    
    # ==================== Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
    
    def get_conversation_history(self, user_id: int) -> List[Dict[str, str]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            if user_id not in self.conversation_history:
                # Ø¬Ù„Ø¨ Ø¢Ø®Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                conversations = self.db.get_user_ai_conversations(user_id, limit=self.max_history_length)
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù€ API
                history = []
                for conv in conversations:
                    history.append({"role": "user", "content": conv['user_message']})
                    history.append({"role": "assistant", "content": conv['ai_response']})
                
                # Ø¹ÙƒØ³ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„ØµØ­ÙŠØ­
                history.reverse()
                self.conversation_history[user_id] = history[-self.max_history_length*2:]  # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            return self.conversation_history.get(user_id, [])
            
        except Exception as e:
            logger.error(f"âŒ Error getting conversation history: {e}")
            return []
    
    def update_conversation_history(self, user_id: int, user_message: str, ai_response: str):
        """ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        try:
            if user_id not in self.conversation_history:
                self.conversation_history[user_id] = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            self.conversation_history[user_id].append({"role": "user", "content": user_message})
            self.conversation_history[user_id].append({"role": "assistant", "content": ai_response})
            
            # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯
            if len(self.conversation_history[user_id]) > self.max_history_length * 2:
                self.conversation_history[user_id] = self.conversation_history[user_id][-self.max_history_length*2:]
                
        except Exception as e:
            logger.error(f"âŒ Error updating conversation history: {e}")
    
    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
    
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        """Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ({remaining} Ø±Ø³Ø§Ù„Ø©).\nðŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„ (ØªÙˆÙ‚ÙŠØª UTC)."
            
            # Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            conversation_history = self.get_conversation_history(user_id)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            if use_gemini and self.gemini_available:
                response = await self._chat_with_gemini(message, conversation_history)
            elif self.openai_available:
                response = await self._chat_with_openai(message, conversation_history)
            else:
                return "âŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø£Ùˆ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† /status."
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            self.update_user_usage(user_id, "ai_chat")
            
            # ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            self.update_conversation_history(user_id, message, response)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.db.save_ai_conversation(user_id, "chat", message, response)
            
            logger.info(f"âœ… AI chat completed for user {user_id}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error in AI chat: {e}")
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø¹Ù… Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø©."
    
    async def _chat_with_gemini(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gemini Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
        try:
            model_name = os.getenv("GEMINI_MODEL", "gemini-pro")
            model = genai.GenerativeModel(model_name)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®
            context_parts = []
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
            system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ¯ÙˆØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. ÙŠØ¬Ø¨ Ø£Ù†:
            1. ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
            2. ØªØ³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
            3. ØªØ¹ØªØ±Ù Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            4. ØªÙƒÙˆÙ† Ù…Ø­Ø§ÙŠØ¯Ø§Ù‹ ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª
            5. ØªÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
            
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø§ Ù„Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø®Ù„Ø§Ù Ø°Ù„Ùƒ."""
            
            context_parts.append(system_prompt)
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            for msg in history[-6:]:  # Ø¢Ø®Ø± 3 ØªØ¨Ø§Ø¯Ù„Ø§Øª ÙÙ‚Ø·
                role = "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" if msg["role"] == "user" else "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
                context_parts.append(f"{role}: {msg['content']}")
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            context_parts.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {message}")
            context_parts.append("Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:")
            
            full_prompt = "\n\n".join(context_parts)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            response = model.generate_content(
                full_prompt,
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 1024,
                }
            )
            
            return response.text.strip()
            
        except Exception as e:
            logger.error(f"âŒ Gemini chat error: {e}")
            raise
    
    async def _chat_with_openai(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI GPT Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
        try:
            client = openai.OpenAI()
            
            messages = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
            messages.append({
                "role": "system",
                "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ØŒ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ØŒ ÙˆÙˆØ¯ÙˆØ¯Ø§Ù‹. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø©."
            })
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            for msg in history[-8:]:  # Ø¢Ø®Ø± 4 ØªØ¨Ø§Ø¯Ù„Ø§Øª
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            messages.append({
                "role": "user",
                "content": message
            })
            
            response = client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview"),
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                top_p=0.9
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ OpenAI chat error: {e}")
            raise
    
    # ==================== Ø®Ø¯Ù…Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± ====================
    
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            allowed, remaining = self.check_user_limit(user_id, "image_gen")
            if not allowed:
                return None, f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ({remaining} ØµÙˆØ±Ø©).\nðŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„."
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            enhanced_prompt = await self._enhance_image_prompt(prompt, style)
            
            image_url = None
            error_message = None
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… DALL-E Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if self.openai_available:
                try:
                    image_url = await self._generate_with_dalle(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with DALL-E for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ DALL-E failed: {e}")
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Stable Diffusion
            if not image_url and self.stability_api_key:
                try:
                    image_url = await self._generate_with_stable_diffusion(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with Stable Diffusion for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Stable Diffusion failed: {e}")
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¯Ø¹Ù… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±
            if not image_url and self.gemini_available:
                try:
                    image_url = await self._generate_with_gemini(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with Gemini for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Gemini image generation failed: {e}")
            
            if image_url:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                self.update_user_usage(user_id, "image_gen")
                
                # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                file_id = self.db.save_generated_file(user_id, "image", prompt, image_url)
                if file_id:
                    logger.info(f"âœ… Image saved to database with ID {file_id}")
                
                return image_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±ØªÙƒ Ø¨Ù†Ø¬Ø§Ø­!"
            else:
                error_message = "âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©. Ø§Ù„Ø®Ø¯Ù…Ø© Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø´ØºÙˆÙ„Ø© Ø£Ùˆ ØºÙŠØ± Ù…ØªØ§Ø­Ø©."
                if not self.openai_available and not self.stability_api_key:
                    error_message += "\nâš ï¸ Ù…ÙØªØ§Ø­ API Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ·."
                
                return None, error_message
            
        except Exception as e:
            logger.error(f"âŒ Error generating image: {e}")
            return None, "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
    
    async def _enhance_image_prompt(self, prompt: str, style: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©"""
        try:
            style_descriptions = {
                "realistic": "ØµÙˆØ±Ø© ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©ØŒ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©",
                "anime": "Ø£Ù†Ù…ÙŠ ÙŠØ§Ø¨Ø§Ù†ÙŠØŒ Ø£Ø³Ù„ÙˆØ¨ Ø±Ø³ÙˆÙ… Ù…ØªØ­Ø±ÙƒØ©ØŒ Ø£Ù„ÙˆØ§Ù† Ø²Ø§Ù‡ÙŠØ©ØŒ Ø¹ÙŠÙˆÙ† ÙƒØ¨ÙŠØ±Ø© ÙˆØªØ¹Ø¨ÙŠØ±Ø§Øª Ù…Ø¨Ø§Ù„Øº ÙÙŠÙ‡Ø§",
                "fantasy": "ÙÙ†ØªØ§Ø²ÙŠØ§ Ø³Ø­Ø±ÙŠØ©ØŒ Ø£Ù„ÙˆØ§Ù† Ø¯Ø±Ø§Ù…Ø§ØªÙŠÙƒÙŠØ©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø®Ù„Ø§Ø¨Ø©ØŒ Ø¬Ùˆ Ø£Ø³Ø·ÙˆØ±ÙŠ",
                "cyberpunk": "Ø³ÙŠØ¨Ø±Ø¨Ø§Ù†ÙƒØŒ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØŒ Ø£Ø¶ÙˆØ§Ø¡ Ù†ÙŠÙˆÙ†ØŒ Ø£Ø¬ÙˆØ§Ø¡ Ø­Ø¶Ø±ÙŠØ© Ù…Ø¸Ù„Ù…Ø©",
                "watercolor": "Ø£Ù„ÙˆØ§Ù† Ù…Ø§Ø¦ÙŠØ©ØŒ ÙØ±Ø´Ø§Ø© ÙÙ†ÙŠØ©ØŒ Ø§Ù†Ø·Ø¨Ø§Ø¹ÙŠØ©ØŒ Ù†Ø§Ø¹Ù…Ø© ÙˆØªØ¬Ø±ÙŠØ¯ÙŠØ©"
            }
            
            style_desc = style_descriptions.get(style, "ØµÙˆØ±Ø© ÙÙ†ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©")
            
            enhancement_prompt = f"""
            Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¬Ø¹Ù„Ù‡ Ù…ÙØµÙ„Ø§Ù‹ ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:
            
            Ø§Ù„ÙˆØµÙ Ø§Ù„Ø£ØµÙ„ÙŠ: {prompt}
            Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {style_desc}
            
            Ø£Ø¶Ù ØªÙØ§ØµÙŠÙ„ Ø¹Ù†:
            1. Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙˆØ§Ù„Ø¸Ù„Ø§Ù„
            2. Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ø¬ÙˆØ§Ø¡
            3. Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù‚Ø©
            4. Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù…Ù†Ø¸ÙˆØ±
            5. Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ÙÙ†ÙŠØ©
            
            Ù‚Ø¯Ù… Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
            """
            
            if self.gemini_available:
                model = genai.GenerativeModel("gemini-pro")
                response = model.generate_content(enhancement_prompt)
                enhanced = response.text.strip()
            else:
                # ÙˆØµÙ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Gemini Ù…ØªØ§Ø­Ø§Ù‹
                enhanced = f"{prompt}, {style_desc}, high quality, detailed, 4k, professional photography"
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            enhanced = enhanced.replace("Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:", "").strip()
            return enhanced if enhanced else f"{prompt}, {style_desc}, high quality"
            
        except Exception as e:
            logger.error(f"âŒ Error enhancing prompt: {e}")
            return f"{prompt}, {style_desc}, high quality"
    
    async def _generate_with_dalle(self, prompt: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DALL-E"""
        try:
            client = openai.OpenAI()
            
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size=os.getenv("IMAGE_SIZE", "1024x1024"),
                quality=os.getenv("IMAGE_QUALITY", "standard"),
                n=1,
                style="vivid"  # Ø£Ùˆ "natural"
            )
            
            return response.data[0].url
            
        except Exception as e:
            logger.error(f"âŒ DALL-E generation error: {e}")
            return None
    
    async def _generate_with_stable_diffusion(self, prompt: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Stable Diffusion"""
        try:
            headers = {
                "Authorization": f"Bearer {self.stability_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "text_prompts": [{"text": prompt, "weight": 1}],
                "cfg_scale": 7,
                "height": 512,
                "width": 512,
                "samples": 1,
                "steps": 30,
                "style_preset": "photographic"  # Ø£Ùˆ "digital-art", "fantasy-art", etc.
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.stable_diffusion_url,
                    headers=headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        if "artifacts" in result and result["artifacts"]:
                            # ØªØ­ÙˆÙŠÙ„ base64 Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· (ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©)
                            image_base64 = result["artifacts"][0]["base64"]
                            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø© Ø§Ø³ØªØ¶Ø§ÙØ©
                            # Ù„Ù„Ù…Ø«Ø§Ù„ØŒ Ù†Ø¹ÙŠØ¯ Ø±Ø§Ø¨Ø· ÙˆÙ‡Ù…ÙŠ
                            return f"https://example.com/generated-image-{hash(prompt)}.png"
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Stable Diffusion error: {e}")
            return None
    
    async def _generate_with_gemini(self, prompt: str) -> Optional[str]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini (Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¯Ø¹Ù…)"""
      
