# ai_manager.py - Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
import os
import logging
import asyncio
import google.generativeai as genai
import openai
import requests
import aiohttp
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Tuple
import base64
import json

logger = logging.getLogger(__name__)

class AIManager:
    """Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„"""
    
    def __init__(self, db):
        self.db = db
        self.setup_apis()
        self.user_limits_cache = {}  # ÙƒØ§Ø´ Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        self.conversation_history = {}  # ÙƒØ§Ø´ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        self.max_history_length = 10  # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
    def setup_apis(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Google Gemini
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                logger.info("âœ… Google Gemini API configured successfully")
            else:
                self.gemini_available = False
                logger.warning("âš ï¸ Google Gemini API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
                logger.info("âœ… OpenAI API configured successfully")
            else:
                self.openai_available = False
                logger.warning("âš ï¸ OpenAI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Luma AI
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            if self.luma_available:
                logger.info("âœ… Luma AI API configured successfully")
            else:
                logger.warning("âš ï¸ Luma AI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup AI APIs: {e}")
            self.gemini_available = False
            self.openai_available = False
            self.luma_available = False
    
    # ==================== Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ ====================
    
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙƒØ§Ø´ Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ÙˆØ¬ÙˆØ¯Ø§Ù‹
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                # Ø¬Ù„Ø¨ Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                    SELECT usage_count FROM ai_usage 
                    WHERE user_id = ? AND service_type = ? AND usage_date = ?
                    ''', (user_id, service_type, today))
                    
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            # ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø§Ù„Ø¨ÙŠØ¦Ø©
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            remaining = limit - current_usage
            
            if current_usage >= limit:
                return False, 0  # ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯
            return True, remaining  # Ù…Ø§ Ø²Ø§Ù„ Ù…ØªØ¨Ù‚ÙŠ
            
        except Exception as e:
            logger.error(f"âŒ Error checking user limit: {e}")
            return True, 999  # Ø§Ù„Ø³Ù…Ø§Ø­ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£
    
    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø´
            current_usage = self.user_limits_cache.get(cache_key, 0)
            self.user_limits_cache[cache_key] = current_usage + 1
            
            # ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                
                conn.commit()
                logger.debug(f"âœ… Updated usage for user {user_id}, service {service_type}")
                return True
                
        except Exception as e:
            logger.error(f"âŒ Error updating user usage: {e}")
            return False
    
    # ==================== Ù†Ø¸Ø§Ù… Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
    
    def get_conversation_history(self, user_id: int) -> List[Dict[str, str]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            if user_id not in self.conversation_history:
                # Ø¬Ù„Ø¨ Ø¢Ø®Ø± Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                conversations = self.db.get_user_ai_conversations(user_id, limit=self.max_history_length)
                
                # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù€ API
                history = []
                for conv in conversations:
                    history.append({"role": "user", "content": conv['user_message']})
                    history.append({"role": "assistant", "content": conv['ai_response']})
                
                # Ø¹ÙƒØ³ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø§Ù„Ø²Ù…Ù†ÙŠ Ø§Ù„ØµØ­ÙŠØ­
                history.reverse()
                self.conversation_history[user_id] = history[-self.max_history_length*2:]  # Ø­ÙØ¸ ÙÙŠ Ø§Ù„ÙƒØ§Ø´
            return self.conversation_history.get(user_id, [])
            
        except Exception as e:
            logger.error(f"âŒ Error getting conversation history: {e}")
            return []
    
    def update_conversation_history(self, user_id: int, user_message: str, ai_response: str):
        """ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        try:
            if user_id not in self.conversation_history:
                self.conversation_history[user_id] = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
            self.conversation_history[user_id].append({"role": "user", "content": user_message})
            self.conversation_history[user_id].append({"role": "assistant", "content": ai_response})
            
            # Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯
            if len(self.conversation_history[user_id]) > self.max_history_length * 2:
                self.conversation_history[user_id] = self.conversation_history[user_id][-self.max_history_length*2:]
                
        except Exception as e:
            logger.error(f"âŒ Error updating conversation history: {e}")
    
    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ====================
    
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        """Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ({remaining} Ø±Ø³Ø§Ù„Ø©).\nğŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„ (ØªÙˆÙ‚ÙŠØª UTC)."
            
            # Ø¬Ù„Ø¨ ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            conversation_history = self.get_conversation_history(user_id)
            
            # Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨
            if use_gemini and self.gemini_available:
                response = await self._chat_with_gemini(message, conversation_history)
            elif self.openai_available:
                response = await self._chat_with_openai(message, conversation_history)
            else:
                return "âŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù„Ø§Ø­Ù‚Ø§Ù‹ Ø£Ùˆ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† /status."
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
            self.update_user_usage(user_id, "ai_chat")
            
            # ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            self.update_conversation_history(user_id, message, response)
            
            # Ø­ÙØ¸ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.db.save_ai_conversation(user_id, "chat", message, response)
            
            logger.info(f"âœ… AI chat completed for user {user_id}")
            return response
            
        except Exception as e:
            logger.error(f"âŒ Error in AI chat: {e}")
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø£Ùˆ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ø¯Ø¹Ù… Ø¥Ø°Ø§ Ø§Ø³ØªÙ…Ø±Øª Ø§Ù„Ù…Ø´ÙƒÙ„Ø©."
    
    async def _chat_with_gemini(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gemini Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
        try:
            model_name = os.getenv("GEMINI_MODEL", "gemini-pro")
            model = genai.GenerativeModel(model_name)
            
            # Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø³ÙŠØ§Ù‚ Ù…Ù† Ø§Ù„ØªØ§Ø±ÙŠØ®
            context_parts = []
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù†Ø¸Ø§Ù…
            system_prompt = """Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ¯ÙˆØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. ÙŠØ¬Ø¨ Ø£Ù†:
            1. ØªÙƒÙˆÙ† Ù…ÙÙŠØ¯Ø§Ù‹ ÙˆÙ…Ø¨Ø§Ø´Ø±Ø§Ù‹ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø§Øª
            2. ØªØ³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø© ÙˆÙˆØ§Ø¶Ø­Ø©
            3. ØªØ¹ØªØ±Ù Ø¹Ù†Ø¯Ù…Ø§ Ù„Ø§ ØªØ¹Ø±Ù Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            4. ØªÙƒÙˆÙ† Ù…Ø­Ø§ÙŠØ¯Ø§Ù‹ ÙˆØ¯ÙˆØ¯Ø§Ù‹ ÙÙŠ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª
            5. ØªÙ‚Ø¯Ù… Ø¥Ø¬Ø§Ø¨Ø§Øª Ø¯Ù‚ÙŠÙ‚Ø© ÙˆÙ…ÙˆØ«ÙˆÙ‚Ø©
            
            Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ù…Ø§ Ù„Ù… ÙŠØ·Ù„Ø¨ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø®Ù„Ø§Ù Ø°Ù„Ùƒ."""
            
            context_parts.append(system_prompt)
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            for msg in history[-6:]:  # Ø¢Ø®Ø± 3 ØªØ¨Ø§Ø¯Ù„Ø§Øª ÙÙ‚Ø·
                role = "Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…" if msg["role"] == "user" else "Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯"
                context_parts.append(f"{role}: {msg['content']}")
            
            # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            context_parts.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {message}")
            context_parts.append("Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:")
            
            full_prompt = "\n\n".join(context_parts)
            
            # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø±Ø¯
            response = model.generate_content(
                full_prompt,
                generation_config={
                    "temperature": 0.7,
                    "top_p": 0.8,
                    "top_k": 40,
                    "max_output_tokens": 1024,
                }
            )
            
            return response.text.strip()
            
        except Exception as e:
            logger.error(f"âŒ Gemini chat error: {e}")
            raise
    
    async def _chat_with_openai(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI GPT Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
        try:
            client = openai.OpenAI()
            
            messages = []
            
            # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
            messages.append({
                "role": "system",
                "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©. ÙƒÙ† Ù…ÙÙŠØ¯Ø§Ù‹ØŒ Ø¯Ù‚ÙŠÙ‚Ø§Ù‹ØŒ ÙˆÙˆØ¯ÙˆØ¯Ø§Ù‹. Ø§Ø³ØªØ®Ø¯Ù… Ù„ØºØ© Ø¹Ø±Ø¨ÙŠØ© Ø³Ù„ÙŠÙ…Ø©."
            })
            
            # Ø¥Ø¶Ø§ÙØ© ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
            for msg in history[-8:]:  # Ø¢Ø®Ø± 4 ØªØ¨Ø§Ø¯Ù„Ø§Øª
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            # Ø¥Ø¶Ø§ÙØ© Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø­Ø§Ù„ÙŠØ©
            messages.append({
                "role": "user",
                "content": message
            })
            
            response = client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4-turbo-preview"),
                messages=messages,
                temperature=0.7,
                max_tokens=1000,
                top_p=0.9
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            logger.error(f"âŒ OpenAI chat error: {e}")
            raise
    
    # ==================== Ø®Ø¯Ù…Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± ====================
    
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            allowed, remaining = self.check_user_limit(user_id, "image_gen")
            if not allowed:
                return None, f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ({remaining} ØµÙˆØ±Ø©).\nğŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„."
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            enhanced_prompt = await self._enhance_image_prompt(prompt, style)
            
            image_url = None
            error_message = None
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 1: Ø§Ø³ØªØ®Ø¯Ø§Ù… DALL-E Ø¥Ø°Ø§ ÙƒØ§Ù† Ù…ØªØ§Ø­Ø§Ù‹
            if self.openai_available:
                try:
                    image_url = await self._generate_with_dalle(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with DALL-E for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ DALL-E failed: {e}")
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 2: Ø§Ø³ØªØ®Ø¯Ø§Ù… Stable Diffusion
            if not image_url and self.stability_api_key:
                try:
                    image_url = await self._generate_with_stable_diffusion(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with Stable Diffusion for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Stable Diffusion failed: {e}")
            
            # Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© 3: Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¯Ø¹Ù… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±
            if not image_url and self.gemini_available:
                try:
                    image_url = await self._generate_with_gemini(enhanced_prompt)
                    if image_url:
                        logger.info(f"âœ… Image generated with Gemini for user {user_id}")
                except Exception as e:
                    logger.warning(f"âš ï¸ Gemini image generation failed: {e}")
            
            if image_url:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                self.update_user_usage(user_id, "image_gen")
                
                # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                file_id = self.db.save_generated_file(user_id, "image", prompt, image_url)
                if file_id:
                    logger.info(f"âœ… Image saved to database with ID {file_id}")
                
                return image_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ±ØªÙƒ Ø¨Ù†Ø¬Ø§Ø­!"
            else:
                error_message = "âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©. Ø§Ù„Ø®Ø¯Ù…Ø© Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø´ØºÙˆÙ„Ø© Ø£Ùˆ ØºÙŠØ± Ù…ØªØ§Ø­Ø©."
                if not self.openai_available and not self.stability_api_key:
                    error_message += "\nâš ï¸ Ù…ÙØªØ§Ø­ API Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± ØºÙŠØ± Ù…Ø¶Ø¨ÙˆØ·."
                
                return None, error_message
            
        except Exception as e:
            logger.error(f"âŒ Error generating image: {e}")
            return None, "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
    
    async def _enhance_image_prompt(self, prompt: str, style: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø©"""
        try:
            style_descriptions = {
                "realistic": "ØµÙˆØ±Ø© ÙÙˆØªÙˆØºØ±Ø§ÙÙŠØ© ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©ØŒ ØªÙØ§ØµÙŠÙ„ Ø¯Ù‚ÙŠÙ‚Ø©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø·Ø¨ÙŠØ¹ÙŠØ©",
                "anime": "Ø£Ù†Ù…ÙŠ ÙŠØ§Ø¨Ø§Ù†ÙŠØŒ Ø£Ø³Ù„ÙˆØ¨ Ø±Ø³ÙˆÙ… Ù…ØªØ­Ø±ÙƒØ©ØŒ Ø£Ù„ÙˆØ§Ù† Ø²Ø§Ù‡ÙŠØ©ØŒ Ø¹ÙŠÙˆÙ† ÙƒØ¨ÙŠØ±Ø© ÙˆØªØ¹Ø¨ÙŠØ±Ø§Øª Ù…Ø¨Ø§Ù„Øº ÙÙŠÙ‡Ø§",
                "fantasy": "ÙÙ†ØªØ§Ø²ÙŠØ§ Ø³Ø­Ø±ÙŠØ©ØŒ Ø£Ù„ÙˆØ§Ù† Ø¯Ø±Ø§Ù…Ø§ØªÙŠÙƒÙŠØ©ØŒ Ø¥Ø¶Ø§Ø¡Ø© Ø®Ù„Ø§Ø¨Ø©ØŒ Ø¬Ùˆ Ø£Ø³Ø·ÙˆØ±ÙŠ",
                "cyberpunk": "Ø³ÙŠØ¨Ø±Ø¨Ø§Ù†ÙƒØŒ Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ ØªÙƒÙ†ÙˆÙ„ÙˆØ¬ÙŠØŒ Ø£Ø¶ÙˆØ§Ø¡ Ù†ÙŠÙˆÙ†ØŒ Ø£Ø¬ÙˆØ§Ø¡ Ø­Ø¶Ø±ÙŠØ© Ù…Ø¸Ù„Ù…Ø©",
                "watercolor": "Ø£Ù„ÙˆØ§Ù† Ù…Ø§Ø¦ÙŠØ©ØŒ ÙØ±Ø´Ø§Ø© ÙÙ†ÙŠØ©ØŒ Ø§Ù†Ø·Ø¨Ø§Ø¹ÙŠØ©ØŒ Ù†Ø§Ø¹Ù…Ø© ÙˆØªØ¬Ø±ÙŠØ¯ÙŠØ©"
            }
            
            style_desc = style_descriptions.get(style, "ØµÙˆØ±Ø© ÙÙ†ÙŠØ© Ø¹Ø§Ù„ÙŠØ© Ø§Ù„Ø¬ÙˆØ¯Ø©")
            
            enhancement_prompt = f"""
            Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ØµÙˆØ±Ø© Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¬Ø¹Ù„Ù‡ Ù…ÙØµÙ„Ø§Ù‹ ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:
            
            Ø§Ù„ÙˆØµÙ Ø§Ù„Ø£ØµÙ„ÙŠ: {prompt}
            Ø§Ù„Ù†Ù…Ø· Ø§Ù„Ù…Ø·Ù„ÙˆØ¨: {style_desc}
            
            Ø£Ø¶Ù ØªÙØ§ØµÙŠÙ„ Ø¹Ù†:
            1. Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙˆØ§Ù„Ø¸Ù„Ø§Ù„
            2. Ø§Ù„Ø£Ù„ÙˆØ§Ù† ÙˆØ§Ù„Ø£Ø¬ÙˆØ§Ø¡
            3. Ø§Ù„ØªÙØ§ØµÙŠÙ„ ÙˆØ§Ù„Ø¯Ù‚Ø©
            4. Ø§Ù„ØªÙƒÙˆÙŠÙ† ÙˆØ§Ù„Ù…Ù†Ø¸ÙˆØ±
            5. Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„ÙÙ†ÙŠØ©
            
            Ù‚Ø¯Ù… Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
            """
            
            if self.gemini_available:
                model = genai.GenerativeModel("gemini-pro")
                response = model.generate_content(enhancement_prompt)
                enhanced = response.text.strip()
            else:
                # ÙˆØµÙ Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Gemini Ù…ØªØ§Ø­Ø§Ù‹
                enhanced = f"{prompt}, {style_desc}, high quality, detailed, 4k, professional photography"
            
            # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†ØªÙŠØ¬Ø©
            enhanced = enhanced.replace("Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©:", "").strip()
            return enhanced if enhanced else f"{prompt}, {style_desc}, high quality"
            
        except Exception as e:
            logger.error(f"âŒ Error enhancing prompt: {e}")
            return f"{prompt}, {style_desc}, high quality"
    
    async def _generate_with_dalle(self, prompt: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… DALL-E"""
        try:
            client = openai.OpenAI()
            
            response = client.images.generate(
                model="dall-e-3",
                prompt=prompt,
                size=os.getenv("IMAGE_SIZE", "1024x1024"),
                quality=os.getenv("IMAGE_QUALITY", "standard"),
                n=1,
                style="vivid"  # Ø£Ùˆ "natural"
            )
            
            return response.data[0].url
            
        except Exception as e:
            logger.error(f"âŒ DALL-E generation error: {e}")
            return None
    
    async def _generate_with_stable_diffusion(self, prompt: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Stable Diffusion"""
        try:
            headers = {
                "Authorization": f"Bearer {self.stability_api_key}",
                "Content-Type": "application/json"
            }
            
            data = {
                "text_prompts": [{"text": prompt, "weight": 1}],
                "cfg_scale": 7,
                "height": 512,
                "width": 512,
                "samples": 1,
                "steps": 30,
                "style_preset": "photographic"  # Ø£Ùˆ "digital-art", "fantasy-art", etc.
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    self.stable_diffusion_url,
                    headers=headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        if "artifacts" in result and result["artifacts"]:
                            # ØªØ­ÙˆÙŠÙ„ base64 Ø¥Ù„Ù‰ Ø±Ø§Ø¨Ø· (ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ù‚ÙŠÙ‚ÙŠØ©ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø©)
                            image_base64 = result["artifacts"][0]["base64"]
                            # Ù‡Ù†Ø§ ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ø®Ø¯Ù…Ø© Ø§Ø³ØªØ¶Ø§ÙØ©
                            # Ù„Ù„Ù…Ø«Ø§Ù„ØŒ Ù†Ø¹ÙŠØ¯ Ø±Ø§Ø¨Ø· ÙˆÙ‡Ù…ÙŠ
                            return f"https://example.com/generated-image-{hash(prompt)}.png"
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Stable Diffusion error: {e}")
            return None
    
    async def _generate_with_gemini(self, prompt: str) -> Optional[str]:
        """Ù…Ø­Ø§ÙˆÙ„Ø© Ø¥Ù†Ø´Ø§Ø¡ ØµÙˆØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini (Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¯Ø¹Ù…)"""
        # Ù…Ù„Ø§Ø­Ø¸Ø©: Gemini Ù„Ø§ ÙŠØ¯Ø¹Ù… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ± Ù…Ø¨Ø§Ø´Ø±Ø© ÙÙŠ ÙˆÙ‚Øª ÙƒØªØ§Ø¨Ø© Ù‡Ø°Ø§ Ø§Ù„ÙƒÙˆØ¯
        # Ù‡Ø°Ù‡ Ø§Ù„Ø¯Ø§Ù„Ø© Ù„Ù„ØªÙˆØ§ÙÙ‚ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ
        return None
    
    # ==================== Ø®Ø¯Ù…Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ====================
    
    async def generate_video(self, user_id: int, prompt: str, image_url: str = None) -> Tuple[Optional[str], str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø§Ù„Ù†Øµ Ø£Ùˆ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Luma AI"""
        try:
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯
            allowed, remaining = self.check_user_limit(user_id, "video_gen")
            if not allowed:
                return None, f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª Ø§Ù„ÙŠÙˆÙ…ÙŠØ© ({remaining} ÙÙŠØ¯ÙŠÙˆ).\nğŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„."
            
            if not self.luma_available:
                return None, "âŒ Ø®Ø¯Ù…Ø© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆÙ‡Ø§Øª ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ÙŠØ±Ø¬Ù‰ Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª API."
            
            video_url = None
            
            if image_url:
                # Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ØµÙˆØ±Ø©
                video_url = await self._generate_video_from_image(prompt, image_url)
            else:
                # Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ù†Øµ
                video_url = await self._generate_video_from_text(prompt)
            
            if video_url:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
                self.update_user_usage(user_id, "video_gen")
                
                # Ø­ÙØ¸ Ø§Ù„Ù…Ù„Ù ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
                file_id = self.db.save_generated_file(user_id, "video", prompt, video_url)
                if file_id:
                    logger.info(f"âœ… Video saved to database with ID {file_id}")
                
                return video_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­! Ù‚Ø¯ ÙŠØ³ØªØºØ±Ù‚ Ø§Ù„ØªØ¬Ù‡ÙŠØ² Ø¨Ø¶Ø¹ Ø¯Ù‚Ø§Ø¦Ù‚."
            else:
                return None, "âŒ ÙØ´Ù„ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. Ø§Ù„Ø®Ø¯Ù…Ø© Ù‚Ø¯ ØªÙƒÙˆÙ† Ù…Ø´ØºÙˆÙ„Ø©. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ù„Ø§Ø­Ù‚Ø§Ù‹."
            
        except Exception as e:
            logger.error(f"âŒ Error generating video: {e}")
            return None, "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ. ÙŠØ±Ø¬Ù‰ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰."
    
    async def _generate_video_from_text(self, prompt: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† Ø§Ù„Ù†Øµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Luma AI"""
        try:
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
            video_prompt = await self._enhance_video_prompt(prompt)
            
            data = {
                "prompt": video_prompt,
                "aspect_ratio": "16:9",
                "duration": 5,
                "prompt_negative": "blurry, low quality, distorted, ugly, text, watermark"
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.lumalabs.ai/dream-machine/v1/generations",
                    headers=headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        generation_id = result.get("id")
                        
                        if generation_id:
                            # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠÙƒØªÙ…Ù„ Ø§Ù„ØªÙˆÙ„ÙŠØ¯
                            video_url = await self._check_luma_generation_status(generation_id)
                            return video_url
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Luma AI text-to-video error: {e}")
            return None
    
    async def _generate_video_from_image(self, prompt: str, image_url: str) -> Optional[str]:
        """Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ù…Ù† ØµÙˆØ±Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Luma AI"""
        try:
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Content-Type": "application/json",
                "Accept": "application/json"
            }
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ
            video_prompt = await self._enhance_video_prompt(prompt)
            
            data = {
                "prompt": video_prompt,
                "image_url": image_url,
                "aspect_ratio": "16:9",
                "duration": 5
            }
            
            async with aiohttp.ClientSession() as session:
                async with session.post(
                    "https://api.lumalabs.ai/dream-machine/v1/generations/image",
                    headers=headers,
                    json=data
                ) as response:
                    if response.status == 200:
                        result = await response.json()
                        generation_id = result.get("id")
                        
                        if generation_id:
                            video_url = await self._check_luma_generation_status(generation_id)
                            return video_url
            
            return None
            
        except Exception as e:
            logger.error(f"âŒ Luma AI image-to-video error: {e}")
            return None
    
    async def _enhance_video_prompt(self, prompt: str) -> str:
        """ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ"""
        try:
            enhancement_prompt = f"""
            Ù‚Ù… Ø¨ØªØ­Ø³ÙŠÙ† ÙˆØµÙ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ØªØ§Ù„ÙŠ Ù„Ø¬Ø¹Ù„Ù‡ Ù…ÙØµÙ„Ø§Ù‹ ÙˆÙ…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ø¥Ù†Ø´Ø§Ø¡ ÙÙŠØ¯ÙŠÙˆ Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ:
            
            Ø§Ù„ÙˆØµÙ Ø§Ù„Ø£ØµÙ„ÙŠ: {prompt}
            
            Ø£Ø¶Ù ØªÙØ§ØµÙŠÙ„ Ø¹Ù†:
            1. Ø§Ù„Ø­Ø±ÙƒØ© ÙˆØ§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠØ©
            2. Ø§Ù„Ø¥Ø¶Ø§Ø¡Ø© ÙˆØ§Ù„ØªØ£Ø«ÙŠØ±Ø§Øª Ø§Ù„Ø¨ØµØ±ÙŠØ©
            3. Ø§Ù„Ø£Ø¬ÙˆØ§Ø¡ ÙˆØ§Ù„Ù…Ø²Ø§Ø¬
            4. Ø§Ù„Ø¬ÙˆØ¯Ø© Ø§Ù„Ø³ÙŠÙ†Ù…Ø§Ø¦ÙŠØ©
            
            Ù‚Ø¯Ù… Ø§Ù„ÙˆØµÙ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ù†ØªØ§Ø¦Ø¬.
            """
            
            if self.gemini_available:
                model = genai.GenerativeModel("gemini-pro")
                response = model.generate_content(enhancement_prompt)
                enhanced = response.text.strip()
            else:
                enhanced = f"{prompt}, cinematic, high quality, smooth motion, dynamic"
            
            return enhanced if enhanced else f"{prompt}, cinematic video"
            
        except Exception as e:
            logger.error(f"âŒ Error enhancing video prompt: {e}")
            return f"{prompt}, cinematic video"
    
    async def _check_luma_generation_status(self, generation_id: str, max_attempts: int = 30) -> Optional[str]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø§Ù„Ø© ØªÙˆÙ„ÙŠØ¯ Luma AI"""
        try:
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Accept": "application/json"
            }
            
            for attempt in range(max_attempts):
                await asyncio.sleep(10)  # Ø§Ù†ØªØ¸Ø§Ø± 10 Ø«ÙˆØ§Ù†ÙŠ Ø¨ÙŠÙ† Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª
                
                async with aiohttp.ClientSession() as session:
                    async with session.get(
                        f"https://api.lumalabs.ai/dream-machine/v1/generations/{generation_id}",
                        headers=headers
                    ) as response:
                        if response.status == 200:
                            result = await response.json()
                            status = result.get("status")
                            
                            if status == "completed":
                                video_url = result.get("video_url")
                                if video_url:
                                    logger.info(f"âœ… Luma generation {generation_id} completed")
                                    return video_url
                            elif status == "failed":
                                error_msg = result.get("error", "Unknown error")
                                logger.error(f"âŒ Luma generation {generation_id} failed: {error_msg}")
                                return None
                            elif status == "processing":
                                logger.info(f"â³ Luma generation {generation_id} still processing (attempt {attempt + 1}/{max_attempts})")
                        else:
                            logger.error(f"âŒ Status check failed for {generation_id}: {response.status}")
            
            logger.warning(f"âš ï¸ Luma generation {generation_id} timed out after {max_attempts} attempts")
            return None
            
        except Exception as e:
            logger.error(f"âŒ Error checking Luma status: {e}")
            return None
    
    # ==================== Ø¯ÙˆØ§Ù„ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ====================
    
    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                
                cursor.execute('''
                SELECT service_type, SUM(usage_count) as total
                FROM ai_usage 
                WHERE user_id = ? AND usage_date = ?
                GROUP BY service_type
                ''', (user_id, today))
                
                stats = {}
                for row in cursor.fetchall():
                    stats[row[0]] = row[1]
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø¯ÙˆØ¯
                limits = {
                    "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                    "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                    "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
                }
                
                # Ø­Ø³Ø§Ø¨ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ
                for service_type, limit in limits.items():
                    used = stats.get(service_type, 0)
                    stats[f"{service_type}_used"] = used
                    stats[f"{service_type}_remaining"] = max(0, limit - used)
                    stats[f"{service_type}_limit"] = limit
                
                return stats
                
        except Exception as e:
            logger.error(f"âŒ Error getting user stats: {e}")
            return {}
    
    def get_available_services(self) -> Dict[str, bool]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        return {
            "chat": self.gemini_available or self.openai_available,
            "image_generation": self.openai_available or bool(self.stability_api_key),
            "video_generation": self.luma_available
        }
    
    def get_service_status(self) -> Dict[str, Dict[str, Any]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ù…ÙØµÙ„Ø©"""
        return {
            "gemini": {
                "available": self.gemini_available,
                "models": ["gemini-pro", "gemini-pro-vision"] if self.gemini_available else []
            },
            "openai": {
                "available": self.openai_available,
                "models": ["gpt-4-turbo", "dall-e-3"] if self.openai_available else []
            },
            "luma_ai": {
                "available": self.luma_available,
                "service": "Dream Machine"
            },
            "stable_diffusion": {
                "available": bool(self.stability_api_key),
                "url": self.stable_diffusion_url if self.stability_api_key else None
            }
        }
    
    def reset_daily_limits(self):
        """Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† ÙƒØ§Ø´ Ø§Ù„Ø­Ø¯ÙˆØ¯ (ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ ÙŠÙˆÙ…ÙŠØ§Ù‹)"""
        try:
            yesterday = (datetime.now() - timedelta(days=1)).strftime('%Y-%m-%d')
            
            # Ù…Ø³Ø­ Ø§Ù„ÙƒØ§Ø´ Ù„Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù‚Ø¯ÙŠÙ…
            keys_to_remove = [k for k in self.user_limits_cache.keys() if yesterday in k]
            for key in keys_to_remove:
                del self.user_limits_cache[key]
            
            logger.info(f"ğŸ§¹ Cleared cache for {len(keys_to_remove)} old entries")
            
        except Exception as e:
            logger.error(f"âŒ Error resetting daily limits: {e}")
    
    def cleanup_old_conversations(self, max_age_days: int = 7):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ù‚Ø¯ÙŠÙ…Ø© Ù…Ù† Ø§Ù„ÙƒØ§Ø´"""
        try:
            # Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ø¨Ø³ÙŠØ·ØŒ ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ù‚Ø¯ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØªØ¨Ø¹ ØªÙˆØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
            current_size = len(self.conversation_history)
            
            # Ù…Ø³Ø­ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø£Ù‚Ø¯Ù… Ù…Ù† X Ø£ÙŠØ§Ù… (ØªÙ†ÙÙŠØ° Ù…Ø¨Ø³Ø·)
            if current_size > 100:  # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙƒØ§Ø´ ÙƒØ¨ÙŠØ±Ø§Ù‹ Ø¬Ø¯Ø§Ù‹
                # Ø­Ø°Ù Ù†ØµÙ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª Ø§Ù„Ø£Ù‚Ù„ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Ù‹
                keys = list(self.conversation_history.keys())
                keys_to_remove = keys[:len(keys)//2]
                
                for key in keys_to_remove:
                    del self.conversation_history[key]
                
                logger.info(f"ğŸ§¹ Cleaned {len(keys_to_remove)} old conversations from cache")
                
        except Exception as e:
            logger.error(f"âŒ Error cleaning old conversations: {e}")