# ai_manager.py - ุงูุฅุตุฏุงุฑ ุงูุนููุงู (Ultimate Edition)
# -----------------------------------------------------------------------------
# ูุฐุง ุงูููู ูู ุงูุนูู ุงููุฏุจุฑ ููุจูุชุ ุชู ุชุตูููู ููููู ูููุงู ููุฑูุงู ููุบุงูุฉ.
#
# ุงูููุฒุงุช ุงูุฌุฏูุฏุฉ ูุงูุชุญุณููุงุช:
# 1. ูุธุงู "ุฃููููุฉ ุงูููุงุฐุฌ ุงููุฑููุฉ" (Hierarchical Model Priority):
#    ูุจุฏุฃ ุจุฃููู ููุฏููุงุช ุงูุฌูู ุงูุซุงูุซ (Gemini 3.0 Preview / Nano Banana)ุ
#    ููุชุฏุฑุฌ ุชููุงุฆูุงู ููุฃุณูู ุนูุฏ ุญุฏูุซ ุฃู ุฎุทุฃ (429/404/500) ูุตููุงู ููููุฏููุงุช ุงููุณุชูุฑุฉ.
#
# 2. ุชุญุณูู ุฐูู ููุฃูุงูุฑ (Advanced Prompt Engineering):
#    ุชู ูุตู ููุทู ุชุญุณูู ุงููุตูุต ูู ุฏูุงู ูุณุชููุฉ ุชุณุชุฎุฏู ุฃููู ููุฏูู ูุชุงุญ
#    ูุชุญููู ุทูุจุงุช ุงููุณุชุฎุฏู ุงูุจุณูุทุฉ ุฅูู ุฃูุตุงู ุงุญุชุฑุงููุฉ ููุตูุฑ ูุงูููุฏูู.
#
# 3. ุชูุซูู ุดุงูู (Comprehensive Documentation):
#    ุดุฑุญ ููุตู ููู ุฏุงูุฉ ูููุทู ูุณูููุฉ ุงูุตูุงูุฉ ูุงูุชุทููุฑ ุงููุณุชูุจูู.
#
# 4. ูุนุงูุฌุฉ ุฃุฎุทุงุก ููุณุนุฉ (Extended Error Handling):
#    ูุธุงู ููุฌ (Logging) ุฏููู ูุฎุจุฑู ุจุงูุถุจุท ุฃู ููุฏูู ูุฌุญ ูุฃู ููุฏูู ูุดู ูููุงุฐุง.
# -----------------------------------------------------------------------------

import os
import logging
import asyncio
import google.generativeai as genai
import openai
import aiohttp
import re  # ููุชุจุฉ ุงูุชุนุงูู ูุน ุงููุตูุต (Regular Expressions)
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple, Union

# ุฅุนุฏุงุฏ ูุธุงู ุงูุชุณุฌูู (Logging)
# ูุณุงุนุฏ ูุฐุง ูู ุชุชุจุน ุงูุฃุฎุทุงุก ุจุฏูุฉ ุฏุงุฎู ููุญุฉ ุชุญูู Railway
logger = logging.getLogger(__name__)

class AIManager:
    """
    ูุฏูุฑ ุฎุฏูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุชูุงูู (AIManager).
    
    ุงููุณุคูููุงุช:
    -----------
    1. ุฅุฏุงุฑุฉ ุงูุงุชุตุงู ูุน Google Gemini API ุจูุฎุชูู ุฅุตุฏุงุฑุงุชู.
    2. ุฅุฏุงุฑุฉ ุงูุงุชุตุงู ูุน OpenAI API (GPT & DALL-E).
    3. ุฅุฏุงุฑุฉ ุงูุงุชุตุงู ูุน Luma Dream Machine API ููููุฏูู.
    4. ุฅุฏุงุฑุฉ ุงูุงุชุตุงู ูุน Stability AI API ูุจุฏูู ููุตูุฑ.
    5. ุชุทุจูู ุญุฏูุฏ ุงูุงุณุชุฎุฏุงู ุงูููููุฉ (Rate Limiting) ูุชุชุจุน ุงุณุชููุงู ุงููุณุชุฎุฏููู.
    6. ุชูุธูู ูุชูุณูู ุงูุฑุฏูุฏ ุงููุงุฏูุฉ ูู ุงูููุฏููุงุช ุงูุฐููุฉ.
    """
    
    def __init__(self, db):
        """
        ุชููุฆุฉ ูุฏูุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนู.
        
        Args:
            db: ูุงุฆู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุงููุณุชุฎุฏู ูุชุฎุฒูู ุงูุณุฌูุงุช ูุงูุญุฏูุฏ.
        """
        self.db = db
        
        # ุชุฎุฒูู ุฌูุณุงุช ุงููุญุงุฏุซุฉ ุงููุดุทุฉ
        # ุงูููุชุงุญ: user_id (int)ุ ุงููููุฉ: ChatSession object
        self.chat_sessions: Dict[int, genai.ChatSession] = {} 
        
        # ุชุนุฑูู ูุงุฆูุฉ ุงูุฃููููุงุช ุงููุตูู ููููุฏููุงุช (The Golden List)
        # ุณูุชู ุงูุชุญูู ูู ุชููุฑ ูุฐู ุงูููุฏููุงุช ูู ุงูุญุณุงุจ ุนูุฏ ุงูุจุฏุก
        self.preferred_models_hierarchy = [
            'gemini-3-pro-preview',        # 1. ุงูุฃููู ุนุงูููุงู (ุงูุฌูู ุงูุซุงูุซ)
            'gemini-3-flash-preview',      # 2. ุงูุฃุณุฑุน ุนุงูููุงู (ุงูุฌูู ุงูุซุงูุซ)
            'nano-banana-pro-preview',     # 3. ููุฏูู ูุชุฎุตุต ุนุงูู ุงูููุงุกุฉ
            'gemini-2.5-flash',            # 4. ุงูุฎูุงุฑ ุงููุณุชูุฑ ูุงูุญุฏูุซ
            'gemini-2.5-pro-preview-tts',  # 5. ุฎูุงุฑ ููู ุจุฏูู
            'gemini-2.0-flash'             # 6. ุงูููุงุฐ ุงูุฃุฎูุฑ (ุงูุงุญุชูุงุทู ุงูุฐูุจู)
        ]
        
        # ุงููุงุฆูุฉ ุงููุนููุฉ ููููุฏููุงุช ุงููุชุงุญุฉ (ุณูุชู ููุคูุง ุจุนุฏ ุงููุญุต)
        self.available_models_chain: List[str] = []
        
        # ุงูููุฏูู ุงูุงูุชุฑุงุถู ุงูุญุงูู
        self.model_name = "gemini-2.5-flash" 
        
        # ูุงุด ูุญูู ููุญุฏูุฏ ูุชูููู ุงุณุชุนูุงูุงุช ูุงุนุฏุฉ ุงูุจูุงูุงุช
        self.user_limits_cache = {}
        
        # ุจุฏุก ุนูููุฉ ุงูุฅุนุฏุงุฏ ูุงูุฑุจุท
        self.setup_apis()
        
    def setup_apis(self):
        """
        ุฅุนุฏุงุฏ ููุงุชูุญ ูุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (APIs) ูุจูุงุก ุณูุณูุฉ ุงูููุฏููุงุช ุงููุชุงุญุฉ.
        ูุฐู ุงูุฏุงูุฉ ุญุงุณูุฉ ูุฃููุง ุชุญุฏุฏ "ุฎุทุฉ ุงููุฌูู" ููุจูุช ุจูุงุกู ุนูู ูุง ูู ูุชุงุญ ูู ุญุณุงุจ ุฌูุฌู.
        """
        logger.info("โ๏ธ ุจุฏุก ุชููุฆุฉ ุฎุฏูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู...")
        
        try:
            # =================================================================
            # 1. ุฅุนุฏุงุฏ Google Gemini ูุจูุงุก ุณูุณูุฉ ุงูุฃููููุงุช
            # =================================================================
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                logger.info("โ ุชู ุงูุงุชุตุงู ุจุฎุฏูุฉ Google Gemini API.")
                
                try:
                    logger.info("๐ ุฌุงุฑู ูุณุญ ุงูููุฏููุงุช ุงููุชุงุญุฉ ูู ุงูุญุณุงุจ ูุชุฑุชูุจ ุงูุฃููููุงุช...")
                    
                    # ุฌูุจ ูู ุงูููุฏููุงุช ุงููุชุงุญุฉ ูู ุงูุญุณุงุจ
                    account_models = [m.name.replace('models/', '') for m in genai.list_models()]
                    logger.info(f"๐ ุงูููุฏููุงุช ุงูุฎุงู ุงูููุฌูุฏุฉ: {len(account_models)} ููุฏูู")
                    
                    # ุจูุงุก ุณูุณูุฉ ุงูููุฏููุงุช ุงููุชุงุญุฉ ูุนููุงู ุจูุงุกู ุนูู ูุงุฆูุชูุง ุงูููุถูุฉ
                    self.available_models_chain = []
                    for preferred in self.preferred_models_hierarchy:
                        if preferred in account_models:
                            self.available_models_chain.append(preferred)
                    
                    # ุฅุฐุง ูู ูุฌุฏ ุฃูุงู ูู ุงูููุฏููุงุช ุงูููุถูุฉ (ุญุงูุฉ ูุงุฏุฑุฉ)ุ ูุถูู 2.5 ู 2.0 ูุฏููุงู
                    if not self.available_models_chain:
                        logger.warning("โ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ุงูููุฏููุงุช ุงูููุถูุฉุ ุณูุชู ุงุณุชุฎุฏุงู ุงููุงุฆูุฉ ุงูุงุญุชูุงุทูุฉ.")
                        self.available_models_chain = ['gemini-2.5-flash', 'gemini-2.0-flash']
                    
                    # ุชุนููู ุงูููุฏูู ุงูุฃุณุงุณู (ุฃูู ูุงุญุฏ ูู ุงูุณูุณูุฉ)
                    self.model_name = self.available_models_chain[0]
                    
                    logger.info(f"๐ ุชู ุจูุงุก ุณูุณูุฉ ุงููุฌูู (Attack Chain): {self.available_models_chain}")
                    logger.info(f"๐ ุงูููุฏูู ุงููุงุฆุฏ ุงูุญุงูู: {self.model_name}")
                        
                except Exception as e:
                    logger.error(f"โ๏ธ ุฎุทุฃ ุฃุซูุงุก ุจูุงุก ุณูุณูุฉ ุงูููุฏููุงุช: {e}")
                    # ูู ุญุงูุฉ ุงููุดู ุงูุชุงูุ ููุฌุฃ ููุถุน ุงูุฃูุงู
                    self.model_name = "gemini-2.5-flash"
                    self.available_models_chain = ["gemini-2.5-flash", "gemini-2.0-flash"]
            else:
                self.gemini_available = False
                logger.critical("โ ููุชุงุญ Google API ุบูุฑ ููุฌูุฏ! (GOOGLE_AI_API_KEY)")
            
            # =================================================================
            # 2. ุฅุนุฏุงุฏ OpenAI (ููุตูุฑ ูุงูุฏุฑุฏุดุฉ ุงูุงุญุชูุงุทูุฉ)
            # =================================================================
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ OpenAI.")
            else:
                self.openai_available = False
                logger.info("โน๏ธ ุฎุฏูุฉ OpenAI ุบูุฑ ููุนูุฉ (ุงูููุชุงุญ ุบูุฑ ููุฌูุฏ).")
            
            # =================================================================
            # 3. ุฅุนุฏุงุฏ Luma AI (ููููุฏูู)
            # =================================================================
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            if self.luma_available:
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ Luma AI (Dream Machine).")
            else:
                logger.info("โน๏ธ ุฎุฏูุฉ Luma AI ููููุฏูู ุบูุฑ ููุนูุฉ.")
            
            # =================================================================
            # 4. ุฅุนุฏุงุฏ Stability AI (ุจุฏูู ููุตูุฑ)
            # =================================================================
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")
            if self.stability_api_key:
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ Stability AI.")
            else:
                logger.info("โน๏ธ ุฎุฏูุฉ Stability AI ุบูุฑ ููุนูุฉ.")

        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ุญุฑุฌ ุบูุฑ ูุชููุน ูู setup_apis: {e}")
            # ุถูุงู ุฃู ุงููุชุบูุฑุงุช ููุง ููู ุญุชู ูู ูุดู ุงูุฅุนุฏุงุฏ ูููุน ุชููู ุงูุจูุช
            self.gemini_available = getattr(self, 'gemini_available', False)
            self.openai_available = getattr(self, 'openai_available', False)
            self.luma_available = getattr(self, 'luma_available', False)
  
    # ==================== ุฃุฏูุงุช ูุนุงูุฌุฉ ุงููุตูุต (Text Utilities) ====================
    
    def clean_response(self, text: str) -> str:
        """
        ุชูุธูู ุงูุฑุฏูุฏ ูู "ุฃููุงุฑ" ุงูููุฏูู (Chain of Thought).
        ุงูููุฏููุงุช ุงูุฌุฏูุฏุฉ ูุซู Gemini 2.5/3.0 ุชููู ูุทุจุงุนุฉ ุฎุทูุงุช ุชูููุฑูุง (THOUGHT: ...)
        ูุจู ุฅุนุทุงุก ุงูุฑุฏ ุงูููุงุฆู. ูุฐู ุงูุฏุงูุฉ ุชุฒูู ุชูู ุงูุฃุฌุฒุงุก ูุชูุฏูู ุชุฌุฑุจุฉ ูุณุชุฎุฏู ูุธููุฉ.
        
        Args:
            text (str): ุงููุต ุงูุฎุงู ุงููุงุฏู ูู ุงูู API.
            
        Returns:
            str: ุงููุต ุงููุธูู ุงูุฌุงูุฒ ููุนุฑุถ.
        """
        if not text:
            return "ุนุฐุฑุงูุ ูู ุฃุณุชุทุน ุชูููู ุฑุฏ ููุงุณุจ ูู ุงูููุช ุงูุญุงูู."
        
        # ุงูุงุญุชูุงุธ ุจุงููุต ุงูุฃุตูู ููุณุฎุฉ ุงุญุชูุงุทูุฉ
        original_text = text
        
        try:
            # 1. ุญุฐู ูุชู ุงูู THOUGHT ุจุงุณุชุฎุฏุงู Regex
            # ูุจุญุซ ุนู ุฃู ูุต ูุจุฏุฃ ุจู THOUGHT: ูููุชูู ุจุณุทุฑูู ูุงุฑุบูู ุฃู ููุงูุฉ ุงููุต
            # Flags: DOTALL (ูุฌุนู ุงูููุทุฉ ุชุดูู ุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉ)ุ IGNORECASE
            clean_text = re.sub(r'THOUGHT:.*?(?=\n\n|\Z)', '', text, flags=re.DOTALL | re.IGNORECASE)
            
            # 2. ุชูุธูู ุฅุถุงูู (ุฅุฒุงูุฉ ุงููุณุงูุงุช ุงูุฒุงุฆุฏุฉ ูุจูุงูุง ุงููููุงุช)
            clean_text = clean_text.replace("THOUGHT:", "").strip()
            
            # 3. ุงูุชุญูู ูู ุงููุชูุฌุฉ (Safety Check)
            # ุฅุฐุง ูุณุญ ุงูุชูุธูู ูู ุงููุต (ุฎุทุฃ ูุง)ุ ูุนูุฏ ุงููุต ุงูุฃุตูู
            if not clean_text or len(clean_text) < 2:
                # logger.warning("โ๏ธ ุนูููุฉ ุงูุชูุธูู ุฃูุฑุบุช ุงูุฑุณุงูุฉุ ุณูุชู ุฅุฑุณุงู ุงููุต ุงูุฃุตูู.")
                return original_text
                
            return clean_text
            
        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ุฃุซูุงุก ุชูุธูู ุงููุต: {e}")
            return original_text

    async def _enhance_prompt_with_ai(self, prompt: str, target_type: str) -> str:
        """
        ุฏุงูุฉ ุฏุงุฎููุฉ ูุณุงุนุฏุฉ ูุชุญุณูู ุงูุฃูุตุงู (Prompt Engineering) ุจุงุณุชุฎุฏุงู ุฃููู ููุฏูู ูุชุงุญ.
        ุชุณุชุฎุฏู ูุฐู ุงูุฏุงูุฉ ูุชุญููู ูุตู ุงููุณุชุฎุฏู ุงูุจุณูุท ุฅูู ูุตู ุงุญุชุฑุงูู ููุตูุฑ ุฃู ุงูููุฏูู.
        
        Args:
            prompt (str): ูุตู ุงููุณุชุฎุฏู ุงูุฃุตูู.
            target_type (str): ููุน ุงูุชุญุณูู ุงููุทููุจ ('image' ุฃู 'video').
            
        Returns:
            str: ุงููุตู ุงููุญุณู ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉ.
        """
        if not self.gemini_available or not self.available_models_chain:
            return prompt # ุฅุฐุง ูู ูุชููุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนูุ ูุฑุฌุน ุงููุต ุงูุฃุตูู
            
        system_instruction = ""
        if target_type == 'image':
            system_instruction = "You are a professional prompt engineer for DALL-E 3. Rewrite the user's prompt to be highly detailed, visual, and artistic in English. Focus on lighting, texture, and composition."
        elif target_type == 'video':
            system_instruction = "You are a cinematographic prompt engineer for Luma Dream Machine. Rewrite the user's prompt to describe a 5-second video scene in English. Focus on motion, camera angles, and atmosphere."
            
        # ูุญุงููุฉ ุงุณุชุฎุฏุงู ุงูููุฏููุงุช ุจุงูุชุฑุชูุจ ููุญุตูู ุนูู ุงูุชุญุณูู
        for model_name in self.available_models_chain:
            try:
                # ุงุณุชุฎุฏุงู generate_content_async ูุฃูู ุฃุณุฑุน ููุง ูุญุชุงุฌ ุณูุงู ูุญุงุฏุซุฉ
                model = genai.GenerativeModel(model_name)
                response = await model.generate_content_async(f"{system_instruction}\n\nUser Prompt: {prompt}")
                
                if response and response.text:
                    enhanced = self.clean_response(response.text)
                    # logger.info(f"โจ ุชู ุชุญุณูู ูุตู {target_type} ุจุงุณุชุฎุฏุงู {model_name}")
                    return enhanced
            except:
                continue # ุชุฌุฑุจุฉ ุงูููุฏูู ุงูุชุงูู ุจุตูุช
                
        return prompt # ุฅุฐุง ูุดู ุงูุฌููุนุ ูุณุชุฎุฏู ุงูุฃุตูู

    # ==================== ุฅุฏุงุฑุฉ ุงูุญุฏูุฏ (Usage Limits) ====================
    
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """
        ูุญุต ูู ููุชูู ุงููุณุชุฎุฏู ุฑุตูุฏุงู ูุงููุงู ูุงุณุชุฎุฏุงู ุงูุฎุฏูุฉ.
        
        Args:
            user_id (int): ูุนุฑู ุงููุณุชุฎุฏู.
            service_type (str): ููุน ุงูุฎุฏูุฉ ('ai_chat', 'image_gen', 'video_gen').
            
        Returns:
            Tuple[bool, int]: (ูุณููุญ/ุบูุฑ ูุณููุญุ ุงูุฑุตูุฏ ุงููุชุจูู).
        """
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # 1. ุงูุชุญูู ูู ุงููุงุด (Fast Path)
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                # 2. ุงูุชุญูู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช (Slow Path)
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        'SELECT usage_count FROM ai_usage WHERE user_id = ? AND service_type = ? AND usage_date = ?', 
                        (user_id, service_type, today)
                    )
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            # ุฅุนุฏุงุฏุงุช ุงูุญุฏูุฏ (ูููู ุชุบููุฑูุง ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ)
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            
            if current_usage >= limit:
                return False, 0
            
            return True, limit - current_usage
            
        except Exception as e:
            logger.error(f"โ Limit Check Error: {e}")
            return True, 999 # ุงูุณูุงุญ ูู ุญุงูุฉ ุชุนุทู ูุงุนุฏุฉ ุงูุจูุงูุงุช (Fail Open)

    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """
        ุฎุตู ุฑุตูุฏ ูู ุงููุณุชุฎุฏู ุจุนุฏ ูุฌุงุญ ุงูุนูููุฉ.
        
        Returns:
            bool: ูุฌุงุญ ุฃู ูุดู ุงูุชุญุฏูุซ.
        """
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ุชุญุฏูุซ ุงููุงุด
            self.user_limits_cache[cache_key] = self.user_limits_cache.get(cache_key, 0) + 1
            
            # ุชุญุฏูุซ ูุงุนุฏุฉ ุงูุจูุงูุงุช
            with self.db.get_connection() as conn:
                conn.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                conn.commit()
            return True
        except Exception as e:
            logger.error(f"โ Usage Update Error: {e}")
            return False

    # ==================== ุฎุฏูุฉ ุงููุญุงุฏุซุฉ (Chat Service) ====================
    
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        """
        ุฅุฌุฑุงุก ูุญุงุฏุซุฉ ุฐููุฉ ุจุงุณุชุฎุฏุงู ุงุณุชุฑุงุชูุฌูุฉ ุชุฏููุฑ ุงูููุฏููุงุช (Model Rotation Strategy).
        
        ุงูููุทู:
        1. ูุญุงูู ุงูุจูุช ุงุณุชุฎุฏุงู ุงูููุฏููุงุช ุงูููุฌูุฏุฉ ูู `self.available_models_chain` ุจุงูุชุฑุชูุจ.
        2. ูุฐู ุงููุงุฆูุฉ ุชุญุชูู ุจุงููุนู ุนูู ุงูููุฏููุงุช ุงููุชูุฏูุฉ (3.0/Nano) ูู ุงูุจุฏุงูุฉ.
        3. ุฅุฐุง ูุดู ููุฏูู ุจุณุจุจ (Quota/Error/Overload)ุ ููุชูู ููุฑุงู ููููุฏูู ุงูุฐู ูููู.
        4. ุฅุฐุง ูุดูุช ุฌููุน ููุฏููุงุช Geminiุ ูุญุงูู ุงุณุชุฎุฏุงู OpenAI (ุฅุฐุง ูุงู ููุนูุงู).
        """
        try:
            # 1. ูุญุต ุงูุฑุตูุฏ
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return "โ ุนุฐุฑุงูุ ููุฏ ุงุณุชูููุช ุฑุตูุฏู ุงููููู ูู ุงูุฑุณุงุฆู. ูุชุฌุฏุฏ ุงูุฑุตูุฏ ุบุฏุงู."
            
            response_text = ""
            success = False
            
            # --- ุงููุณุงุฑ ุงูุฃูู: Google Gemini (ุงูุณูุณูุฉ ุงููุงููุฉ) ---
            if use_gemini and self.gemini_available and self.available_models_chain:
                
                # ุงูุชูุฑุงุฑ ุนุจุฑ ุณูุณูุฉ ุงูููุฏููุงุช (ูู ุงูุฃููู ุฅูู ุงูุฃุถุนู/ุงูุฃูุฏู)
                for model_name in self.available_models_chain:
                    try:
                        # logger.info(f"๐ ูุญุงููุฉ ุงูุฑุฏ ุจุงุณุชุฎุฏุงู ุงูููุฏูู: {model_name} ...")
                        
                        # ุฅุนุฏุงุฏ ุงูุฌูุณุฉ ููุฐุง ุงููุณุชุฎุฏู ูุน ูุฐุง ุงูููุฏูู ุชุญุฏูุฏุงู
                        # ููุงุญุธุฉ: ูููู ุจุฅูุดุงุก ูุงุฆู GenerativeModel ุฌุฏูุฏ ููู ูุญุงููุฉ ูุถูุงู ุนุฏู ุชุฏุงุฎู ุงูุฅุนุฏุงุฏุงุช
                        current_model = genai.GenerativeModel(model_name)
                        
                        # ุงูุชุญูู ูู ููุงู ุฌูุณุฉ ุณุงุจูุฉ ููุฐุง ุงููุณุชุฎุฏู ูุชูุงููุฉุ
                        # ููุชุจุณูุท ูุถูุงู ุงููุฌุงุญ ูู ุญุงูุฉ ุงูุชุจุฏูู ุจูู ุงูููุฏููุงุชุ ุณูุณุชุฎุฏู generate_content_async
                        # ุฃู ููุดุฆ ุฏุฑุฏุดุฉ ุฌุฏูุฏุฉ. ููุญูุงุธ ุนูู ุงูุณูุงู (Context)ุ ุงูุญู ุงูุฃูุซู ูู ุฅุฏุงุฑุฉ ุงูุณุฌู ูุฏููุงูุ
                        # ูููู ููุง ุณูุนุชูุฏ ุนูู ููุชุจุฉ ุฌูุฌู ูุฅุฏุงุฑุฉ ุงูุฏุฑุฏุดุฉุ ูุฅุฐุง ูุดูุช ูุนูุฏ ุงูุจุฏุก.
                        
                        if user_id not in self.chat_sessions:
                            # ุจุฏุก ุฌูุณุฉ ุฌุฏูุฏุฉ
                            chat = current_model.start_chat(history=[
                                {"role": "user", "parts": ["ุฃูุช ูุณุงุนุฏ ุฐูู ููููุฏ. ุฑุฏ ูุจุงุดุฑุฉ ุจุงูุนุฑุจูุฉ."]},
                                {"role": "model", "parts": ["ุญุณูุงู."]}
                            ])
                            self.chat_sessions[user_id] = chat
                        else:
                            # ุชุญุฏูุซ ุงูููุฏูู ููุฌูุณุฉ ุงูุญุงููุฉ (ุฎุฏุนุฉ ุจุฑูุฌูุฉ ููุชุจุฏูู ุฏูู ููุฏุงู ุงูุฐุงูุฑุฉ ุฅุฐุง ุฃููู)
                            # ูู ููุชุจุฉ ุฌูุฌู ุงูุญุงููุฉุ ูุฏ ูุชุทูุจ ุงูุฃูุฑ ุจุฏุก ุฌูุณุฉ ุฌุฏูุฏุฉ ูุชูุฑูุฑ ุงูุชุงุฑูุฎ.
                            # ููุชุฌูุจ ุงูุชุนููุฏุ ุณูุญุงูู ุงูุฅุฑุณุงู ุนุจุฑ ุงูุฌูุณุฉ ุงูุญุงููุฉุ ูุฅุฐุง ูุดูุช ููุดุฆ ุฌุฏูุฏุฉ.
                            pass

                        chat_session = self.chat_sessions[user_id]
                        
                        # ูุญุงููุฉ ุงูุฅุฑุณุงู
                        # ุงุณุชุฎุฏุงู timeout ูุชุฌูุจ ุงูุงูุชุธุงุฑ ุงูุทููู
                        response = await asyncio.wait_for(
                            chat_session.send_message_async(message), 
                            timeout=60.0
                        )
                        
                        if response and response.text:
                            response_text = self.clean_response(response.text)
                            success = True
                            # logger.info(f"โ ูุฌุงุญ ุงูุฑุฏ ูู ุงูููุฏูู: {model_name}")
                            
                            # ุฅุฐุง ูุฌุญูุงุ ูุฎุฑุฌ ูู ุงูุญููุฉ (ูุง ุฏุงุนู ูุชุฌุฑุจุฉ ุจุงูู ุงูููุฏููุงุช)
                            break 
                            
                    except Exception as e:
                        # ุชุญููู ุงูุฎุทุฃ ูุชุญุฏูุฏ ูุง ุฅุฐุง ูุงู ูุฌุจ ุงููุชุงุจุนุฉ
                        error_msg = str(e).lower()
                        is_quota_error = "429" in error_msg or "quota" in error_msg or "resource" in error_msg
                        is_not_found = "404" in error_msg or "not found" in error_msg
                        
                        if is_quota_error:
                            logger.warning(f"โ๏ธ ุชุฌุงูุฒ ุญุตุฉ ุงูููุฏูู {model_name}. ุงูุงูุชูุงู ููุชุงูู...")
                        elif is_not_found:
                            logger.error(f"โ ุงูููุฏูู {model_name} ุบูุฑ ููุฌูุฏ (404). ุฅุฒุงูุชู ูู ุงููุงุฆูุฉ...")
                            # ูููููุง ุฅุฒุงูุชู ูู ุงููุงุฆูุฉ ุงููุณุชูุจููุฉ ูุชุญุณูู ุงูุฃุฏุงุก
                        else:
                            logger.warning(f"โ๏ธ ุฎุทุฃ ุบูุฑ ูุชููุน ูู {model_name}: {e}")
                        
                        # ุฅุนุงุฏุฉ ุชุนููู ุงูุฌูุณุฉ ูููุณุชุฎุฏู ูุฃู ุงูููุฏูู ุงูุญุงูู ูุดู
                        if user_id in self.chat_sessions:
                            del self.chat_sessions[user_id]
                        
                        continue # ุงูุงูุชูุงู ููููุฏูู ุงูุชุงูู ูู ุงูุญููุฉ

            # --- ุงููุณุงุฑ ุงูุซุงูู: OpenAI (ุงูุงุญุชูุงุทู ุงูููุงุฆู) ---
            if not success and self.openai_available:
                try:
                    logger.info("๐ ุงูุงูุชูุงู ุฅูู OpenAI (GPT-4o-mini) ูุญู ุฃุฎูุฑ...")
                    client = openai.OpenAI()
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": message}]
                    )
                    response_text = response.choices[0].message.content
                    success = True
                except Exception as e:
                    logger.error(f"โ ูุดู OpenAI ุฃูุถุงู: {e}")

            # --- ุงููุชูุฌุฉ ุงูููุงุฆูุฉ ---
            if success:
                self.update_user_usage(user_id, "ai_chat")
                self.db.save_ai_conversation(user_id, "chat", message, response_text)
                return response_text
            else:
                return "โ๏ธ ุนุฐุฑุงูุ ุฌููุน ุฎูุงุฏู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุดุบููุฉ ุญุงููุงู (Google & OpenAI). ูุฑุฌู ุงููุญุงููุฉ ุจุนุฏ ูููู."
            
        except Exception as e:
            logger.error(f"โ General Chat Error: {e}")
            return "โ๏ธ ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ูู ุงููุธุงู."

    # ==================== ุฎุฏูุฉ ุงูุตูุฑ (Image Gen Service) ====================
    
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        """
        ุชูููุฏ ุงูุตูุฑ ุจุงุณุชุฎุฏุงู DALL-E 3 ุฃู Stability AI.
        ูุชู ุชุญุณูู ุงููุตู ุฃููุงู ุจุงุณุชุฎุฏุงู ููุฏููุงุช Gemini ุงููุชูุฏูุฉ (Nano/3.0).
        """
        try:
            # 1. ุงูุชุญูู ูู ุงูุญุฏูุฏ
            allowed, _ = self.check_user_limit(user_id, "image_gen")
            if not allowed: return None, "โ ุงูุชูู ุฑุตูุฏ ุงูุตูุฑ ุงููููู."
            
            # 2. ุชุญุณูู ุงููุตู (Advanced Prompt Engineering)
            # ูุณุชุฎุฏู ุฏุงูุฉ ุงูุชุญุณูู ุงููุฎุตุตุฉ ุงูุชู ุชุณุชุบู ุฃููู ููุฏูู ูุชุงุญ
            enhanced_prompt = await self._enhance_prompt_with_ai(prompt, 'image')
            
            image_url = None
            
            # 3. ุงููุญุงููุฉ ุงูุฃููู: OpenAI DALL-E 3
            if self.openai_available:
                try:
                    # logger.info("๐จ ุฌุงุฑู ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู DALL-E 3...")
                    client = openai.OpenAI()
                    response = client.images.generate(
                        model="dall-e-3",
                        prompt=enhanced_prompt[:1000], # DALL-E limit
                        size="1024x1024",
                        quality="standard",
                        n=1
                    )
                    image_url = response.data[0].url
                except Exception as e:
                    logger.warning(f"โ ูุดู DALL-E 3: {e}")

            # 4. ุงููุญุงููุฉ ุงูุซุงููุฉ: Stability AI (ุฅุฐุง ูุดู DALL-E)
            if not image_url and self.stability_api_key:
                try:
                    # logger.info("๐จ ุฌุงุฑู ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู Stability AI...")
                    headers = {
                        "Authorization": f"Bearer {self.stability_api_key}",
                        "Content-Type": "application/json",
                        "Accept": "application/json"
                    }
                    data = {
                        "text_prompts": [{"text": enhanced_prompt, "weight": 1}],
                        "cfg_scale": 7,
                        "height": 512,
                        "width": 512,
                        "samples": 1
                    }
                    async with aiohttp.ClientSession() as session:
                        async with session.post(self.stable_diffusion_url, headers=headers, json=data) as resp:
                            if resp.status == 200:
                                # ููุงุญุธุฉ: Stability ูุนูุฏ ุงูุตูุฑุฉ ูุจูุงูุงุช Base64 ูููุณ ุฑุงุจุท
                                # ุจูุง ุฃููุง ูุง ูููู ููุฏ ุฑูุน ุงูุตูุฑ (Upload Service) ููุงุ ุณูุนุชุจุฑู ูุฌุงุญุงู
                                # ููุนูุฏ ุฑุณุงูุฉ ุชูุถูุญูุฉ. ูู ุงูุชุทุจูู ุงููุนูู ูุฌุจ ูู ุชุดููุฑ Base64 ูุฑูุนู.
                                return None, "โ๏ธ ุชู ุฅูุดุงุก ุงูุตูุฑุฉ ุจูุฌุงุญ (Stability)ุ ูููู ุงููุธุงู ูุญุชุงุฌ ุฎุฏูุฉ ุชุฎุฒูู ูุนุฑุถูุง."
                            else:
                                logger.error(f"Stability Error: {await resp.text()}")
                except Exception as e:
                    logger.warning(f"โ ูุดู Stability AI: {e}")

            # 5. ูุนุงูุฌุฉ ุงููุชูุฌุฉ
            if image_url:
                self.update_user_usage(user_id, "image_gen")
                self.db.save_generated_file(user_id, "image", prompt, image_url)
                return image_url, "โ ุชู ุฅูุดุงุก ุงูุตูุฑุฉ ุจูุฌุงุญ"
            
            return None, "โ ูุดู ุฅูุดุงุก ุงูุตูุฑุฉ. ุชุฃูุฏ ูู ุชููุฑ ุฑุตูุฏ ูู OpenAI ุฃู Stability."

        except Exception as e:
            logger.error(f"โ Image Gen Error: {e}")
            return None, "ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ูู ุฎุฏูุฉ ุงูุตูุฑ."

    # ==================== ุฎุฏูุฉ ุงูููุฏูู (Video Gen Service) ====================
    
    async def generate_video(self, user_id: int, prompt: str, image_url: str = None) -> Tuple[Optional[str], str]:
        """
        ุชูููุฏ ููุฏูู ุจุงุณุชุฎุฏุงู Luma Dream Machine.
        ูุชู ุชุญุณูู ุงููุตู ุฃููุงู ููููู ุณูููุงุฆูุงู (Cinematic Prompt).
        """
        try:
            # 1. ุงูุชุญูู ูู ุงูุญุฏูุฏ
            allowed, _ = self.check_user_limit(user_id, "video_gen")
            if not allowed: return None, "โ ุงูุชูู ุฑุตูุฏ ุงูููุฏูู ุงููููู."
            
            if not self.luma_available:
                return None, "โ ุฎุฏูุฉ ุงูููุฏูู ุบูุฑ ููุนูุฉ (LUMAAI_API_KEY ุบูุฑ ููุฌูุฏ)."

            # 2. ุชุญุณูู ุงููุตู ููููุฏูู
            enhanced_prompt = await self._enhance_prompt_with_ai(prompt, 'video')

            # 3. ุฅุนุฏุงุฏ ุงูุทูุจ
            url = "https://api.lumalabs.ai/dream-machine/v1/generations"
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "prompt": enhanced_prompt,
                "aspect_ratio": "16:9"
            }
            
            if image_url:
                url = "https://api.lumalabs.ai/dream-machine/v1/generations/image"
                payload["image_url"] = image_url
            
            # 4. ุฅุฑุณุงู ุงูุทูุจ (Async HTTP)
            async with aiohttp.ClientSession() as session:
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status not in [200, 201]:
                        err_text = await response.text()
                        logger.error(f"Luma API Error: {response.status} - {err_text}")
                        return None, f"โ ุฎุทุฃ ูู Luma: {response.status}"
                    
                    data = await response.json()
                    gen_id = data.get("id")
                    
                    if not gen_id:
                        return None, "โ ูู ูุชู ุงุณุชูุงู ูุนุฑู ุงูููุฏูู ูู ุงูุฎุงุฏู."
                    
                    # 5. ุงูุชุธุงุฑ ุงููุชูุฌุฉ (Polling Loop)
                    # ููุชุธุฑ ููุฏุฉ ุชุตู ุฅูู 5 ุฏูุงุฆู (60 ูุญุงููุฉ * 5 ุซูุงูู)
                    for _ in range(60):
                        await asyncio.sleep(5)
                        async with session.get(f"{url}/{gen_id}", headers=headers) as check_resp:
                            if check_resp.status == 200:
                                status_data = await check_resp.json()
                                state = status_data.get("state")
                                
                                if state == "completed":
                                    video_url = status_data.get("assets", {}).get("video")
                                    if video_url:
                                        self.update_user_usage(user_id, "video_gen")
                                        self.db.save_generated_file(user_id, "video", prompt, video_url)
                                        return video_url, "โ ุชู ุฅูุดุงุก ุงูููุฏูู ุจูุฌุงุญ!"
                                elif state == "failed":
                                    failure_reason = status_data.get('failure_reason', 'ุบูุฑ ูุนุฑูู')
                                    return None, f"โ ูุดู ุชูููุฏ ุงูููุฏูู: {failure_reason}"
            
            return None, "โ๏ธ ุงุณุชุบุฑู ุงูููุฏูู ููุชุงู ุทูููุงู ุฌุฏุงู (Time out)ุ ุณูุชู ุฅุดุนุงุฑู ุนูุฏ ุงูุชูุงูู."

        except Exception as e:
            logger.error(f"โ Video Error: {e}")
            return None, "ุญุฏุซ ุฎุทุฃ ุชููู ูู ุฎุฏูุฉ ุงูููุฏูู."

    # ==================== ุฏูุงู ูุณุงุนุฏุฉ ุนุงูุฉ (Utility Functions) ====================
    
    def get_available_services(self) -> Dict[str, bool]:
        """
        ุฅุฑุฌุงุน ุชูุฑูุฑ ุนู ุญุงูุฉ ุงูุฎุฏูุงุช ุงููุชุงุญุฉ ุญุงููุงู.
        ูุณุชุฎุฏู ูุฐุง ูู ุฃูุฑ /status ูุนุฑุถ ุญุงูุฉ ุงูุจูุช.
        """
        return {
            "chat": self.gemini_available or self.openai_available,
            "image_generation": self.openai_available or bool(self.stability_api_key),
            "video_generation": self.luma_available
        }
        
    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        """
        ุฅุฑุฌุงุน ุฅุญุตุงุฆูุงุช ุงุณุชุฎุฏุงู ุงููุณุชุฎุฏู ููููู ุงูุญุงูู.
        ูุณุชุฎุฏู ูุฐุง ูู ุฃูุฑ /mystats.
        """
        stats = {}
        for s_type in ["ai_chat", "image_gen", "video_gen"]:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{s_type}"
            stats[s_type] = self.user_limits_cache.get(cache_key, 0)
        return stats
