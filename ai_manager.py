# ai_manager.py - Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„
import os
import logging
import asyncio
import google.generativeai as genai
import openai
import requests
import aiohttp
from datetime import datetime, timedelta
from typing import Optional, List, Dict, Any, Tuple
import base64
import json

logger = logging.getLogger(__name__)

class AIManager:
    """Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„"""
    
    def __init__(self, db):
        self.db = db
        self.setup_apis()
        self.user_limits_cache = {}  # ÙƒØ§Ø´ Ù„Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
        self.conversation_history = {}  # ÙƒØ§Ø´ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª
        self.max_history_length = 10  # Ø£Ù‚ØµÙ‰ Ø·ÙˆÙ„ Ù„ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©
        
    def setup_apis(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ ÙˆØ§Ø¬Ù‡Ø§Øª Ø¨Ø±Ù…Ø¬Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª"""
        try:
            # Ø¥Ø¹Ø¯Ø§Ø¯ Google Gemini
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                logger.info("âœ… Google Gemini API configured successfully")
            else:
                self.gemini_available = False
                logger.warning("âš ï¸ Google Gemini API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
                logger.info("âœ… OpenAI API configured successfully")
            else:
                self.openai_available = False
                logger.warning("âš ï¸ OpenAI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Luma AI
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            if self.luma_available:
                logger.info("âœ… Luma AI API configured successfully")
            else:
                logger.warning("âš ï¸ Luma AI API key not found")
            
            # Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø¨Ø¯ÙŠÙ„Ø©
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            
        except Exception as e:
            logger.error(f"âŒ Failed to setup AI APIs: {e}")
            self.gemini_available = False
            self.openai_available = False
            self.luma_available = False
    
    # ==================== Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø¯ÙˆØ¯ ====================
    
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø­Ø¯ÙˆØ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('''
                    SELECT usage_count FROM ai_usage 
                    WHERE user_id = ? AND service_type = ? AND usage_date = ?
                    ''', (user_id, service_type, today))
                    
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            remaining = limit - current_usage
            
            if current_usage >= limit:
                return False, 0
            return True, remaining
            
        except Exception as e:
            logger.error(f"âŒ Error checking user limit: {e}")
            return True, 999
    
    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            current_usage = self.user_limits_cache.get(cache_key, 0)
            self.user_limits_cache[cache_key] = current_usage + 1
            
            with self.db.get_connection() as conn:
                cursor = conn.cursor()
                cursor.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                conn.commit()
                return True
        except Exception as e:
            logger.error(f"âŒ Error updating user usage: {e}")
            return False
    
    def get_conversation_history(self, user_id: int) -> List[Dict[str, str]]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ ØªØ§Ø±ÙŠØ® Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            if user_id not in self.conversation_history:
                conversations = self.db.get_user_ai_conversations(user_id, limit=self.max_history_length)
                history = []
                for conv in conversations:
                    history.append({"role": "user", "content": conv['user_message']})
                    history.append({"role": "assistant", "content": conv['ai_response']})
                history.reverse()
                self.conversation_history[user_id] = history[-self.max_history_length*2:]
            return self.conversation_history.get(user_id, [])
        except Exception as e:
            logger.error(f"âŒ Error getting conversation history: {e}")
            return []
    
    def update_conversation_history(self, user_id: int, user_message: str, ai_response: str):
        """ØªØ­Ø¯ÙŠØ« ØªØ§Ø±ÙŠØ® Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©"""
        try:
            if user_id not in self.conversation_history:
                self.conversation_history[user_id] = []
            self.conversation_history[user_id].append({"role": "user", "content": user_message})
            self.conversation_history[user_id].append({"role": "assistant", "content": ai_response})
            if len(self.conversation_history[user_id]) > self.max_history_length * 2:
                self.conversation_history[user_id] = self.conversation_history[user_id][-self.max_history_length*2:]
        except Exception as e:
            logger.error(f"âŒ Error updating conversation history: {e}")
    
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        """Ø¯Ø±Ø¯Ø´Ø© Ù…Ø¹ Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"""
        try:
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return f"âŒ Ù„Ù‚Ø¯ Ø§Ø³ØªØ®Ø¯Ù…Øª Ø¬Ù…ÙŠØ¹ Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ÙŠÙˆÙ…ÙŠØ©.\nğŸ”„ ÙŠØªÙ… ØªØ¬Ø¯ÙŠØ¯ Ø§Ù„Ø­Ø¯ÙˆØ¯ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¨Ø¹Ø¯ Ù…Ù†ØªØµÙ Ø§Ù„Ù„ÙŠÙ„."
            
            conversation_history = self.get_conversation_history(user_id)
            
            if use_gemini and self.gemini_available:
                response = await self._chat_with_gemini(message, conversation_history)
            elif self.openai_available:
                response = await self._chat_with_openai(message, conversation_history)
            else:
                return "âŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø¯Ø±Ø¯Ø´Ø© ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹."
            
            self.update_user_usage(user_id, "ai_chat")
            self.update_conversation_history(user_id, message, response)
            self.db.save_ai_conversation(user_id, "chat", message, response)
            return response
        except Exception as e:
            logger.error(f"âŒ Error in AI chat: {e}")
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ Ø£Ø«Ù†Ø§Ø¡ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø·Ù„Ø¨Ùƒ. Ø­Ø§ÙˆÙ„ Ù…Ø¬Ø¯Ø¯Ø§Ù‹."
    
        async def _chat_with_gemini(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… Google Gemini Ù„Ù„Ø¯Ø±Ø¯Ø´Ø© - Ù†Ø³Ø®Ø© Ù…Ø³ØªÙ‚Ø±Ø© ÙˆÙ…Ø­Ø¯Ø«Ø©"""
        try:
            # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ø³Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø¨Ø§Ø´Ø±Ø© Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ØªÙˆØ§ÙÙ‚
            model_name = "gemini-1.5-flash"
            model = genai.GenerativeModel(model_name=model_name)
            
            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„
            system_instruction = "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ØŒ ØªØ¬ÙŠØ¨ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨ÙˆØ¶ÙˆØ­ ÙˆØ§Ø®ØªØµØ§Ø±."
            
            # Ø¯Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ù…Ø¹ Ø±Ø³Ø§Ù„Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…
            full_prompt = f"{system_instruction}\n\nØ§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {message}"
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„Ø·Ù„Ø¨ ÙÙŠ thread Ù…Ù†ÙØµÙ„ Ù„ØªØ¬Ù†Ø¨ ØªØ¹Ù„ÙŠÙ‚ Ø§Ù„Ø¨ÙˆØª (Ù„Ø£Ù† Ù…ÙƒØªØ¨Ø© Google Ù„ÙŠØ³Øª async Ø¨Ø§Ù„ÙƒØ§Ù…Ù„)
            response = await asyncio.to_thread(model.generate_content, full_prompt)
            
            if response and response.text:
                return response.text.strip()
            else:
                return "Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù… Ø£ØªÙ…ÙƒÙ† Ù…Ù† ØªÙˆÙ„ÙŠØ¯ Ø±Ø¯ Ø­Ø§Ù„ÙŠØ§Ù‹."
                
        except Exception as e:
            logger.error(f"âŒ Gemini chat error: {e}")
            # Ø®Ø·Ø© Ø¨Ø¯ÙŠÙ„Ø©: Ø¥Ø°Ø§ ÙØ´Ù„ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ø­Ø¯Ø¯ØŒ Ù†Ø­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ
            try:
                fallback_model = genai.GenerativeModel("gemini-pro")
                fallback_res = await asyncio.to_thread(fallback_model.generate_content, message)
                return fallback_res.text.strip()
            except:
                raise e
    
    async def _chat_with_openai(self, message: str, history: List[Dict]) -> str:
        """Ø§Ø³ØªØ®Ø¯Ø§Ù… OpenAI GPT Ù„Ù„Ø¯Ø±Ø¯Ø´Ø©"""
        try:
            client = openai.OpenAI()
            messages = [{"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆØ¯ÙˆØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."}]
            for msg in history[-6:]:
                messages.append(msg)
            messages.append({"role": "user", "content": message})
            
            response = client.chat.completions.create(
                model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
                messages=messages,
                temperature=0.7
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            logger.error(f"âŒ OpenAI chat error: {e}")
            raise

    # [Ù…Ù„Ø§Ø­Ø¸Ø©: ØªÙ… Ø§Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Ø¯ÙˆØ§Ù„ Ø§Ù„ØµÙˆØ± ÙˆØ§Ù„ÙÙŠØ¯ÙŠÙˆ ÙƒÙ…Ø§ Ù‡ÙŠ Ù„Ø¶Ù…Ø§Ù† Ø¹Ø¯Ù… Ø­Ø¯ÙˆØ« Ø£Ø®Ø·Ø§Ø¡ Ø£Ø®Ø±Ù‰]
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        # ÙƒÙˆØ¯ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±...
        return None, "Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ± ØªØ­Øª Ø§Ù„ØµÙŠØ§Ù†Ø©"

    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        return {}
    
    def get_available_services(self) -> Dict[str, bool]:
        return {"chat": self.gemini_available or self.openai_available}
