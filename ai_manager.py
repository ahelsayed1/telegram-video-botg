# ai_manager.py - Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø© ÙˆØ§Ù„Ù…ØµØ­Ø­Ø© (Fix 404 + Memory + All Features)
import os
import logging
import asyncio
import google.generativeai as genai
import openai
import aiohttp
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple

logger = logging.getLogger(__name__)

class AIManager:
    """
    Ù…Ø¯ÙŠØ± Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…ØªÙƒØ§Ù…Ù„.
    ÙŠØ¯Ø¹Ù…:
    1. Ù…Ø­Ø§Ø¯Ø«Ø© Gemini Ù…Ø¹ Ø°Ø§ÙƒØ±Ø© (Context Aware).
    2. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØµÙˆØ± (OpenAI / Stability).
    3. ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Luma AI).
    4. Ø¥Ø¯Ø§Ø±Ø© Ø­Ø¯ÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ÙŠÙˆÙ…ÙŠ.
    """
    
    def __init__(self, db):
        self.db = db
        # Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø§Øª: Ù‚Ø§Ù…ÙˆØ³ ÙŠØ±Ø¨Ø· Ù…Ø¹Ø±Ù Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ø¨Ø¬Ù„Ø³Ø© Ø§Ù„Ø´Ø§Øª
        self.chat_sessions: Dict[int, genai.ChatSession] = {} 
        self.model_name = "gemini-1.5-flash" # Ø§Ù„Ù‚ÙŠÙ…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø§Ù„Ø¢Ù…Ù†Ø©
        self.user_limits_cache = {}
        self.setup_apis()
        
    def setup_apis(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ÙØ§ØªÙŠØ­ ÙˆØ§Ø®ØªÙŠØ§Ø± Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù…Ù†Ø§Ø³Ø¨"""
        try:
            # 1. Ø¥Ø¹Ø¯Ø§Ø¯ Google Gemini
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                
                # --- Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø®Ø·Ø£ 404: Ø§Ø®ØªÙŠØ§Ø± Ø¯Ù‚ÙŠÙ‚ Ù„Ù„Ù…ÙˆØ¯ÙŠÙ„ ---
                try:
                    logger.info("ðŸ” Ø¬Ø§Ø±ÙŠ ÙØ­Øµ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Gemini Ø§Ù„Ù…ØªØ§Ø­Ø©...")
                    all_models = [m.name.replace('models/', '') for m in genai.list_models()]
                    logger.info(f"ðŸ“‹ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø©: {all_models}")
                    
                    # Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…ÙØ¶Ù„Ø© (Ø¨Ø§Ù„ØªØ±ØªÙŠØ¨ Ù…Ù† Ø§Ù„Ø£Ø³Ø±Ø¹ ÙˆØ§Ù„Ø£Ø­Ø¯Ø«)
                    # ØªÙ… Ø¥Ø²Ø§Ù„Ø© 'gemini-pro' Ø§Ù„Ù‚Ø¯ÙŠÙ… Ù„ØªØ¬Ù†Ø¨ Ø§Ù„Ø®Ø·Ø£ 404
                    preferred_models = [
                        'gemini-1.5-flash',       # Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø£ÙˆÙ„: Ø³Ø±ÙŠØ¹ Ø¬Ø¯Ø§Ù‹ ÙˆÙ…Ø¬Ø§Ù†ÙŠ
                        'gemini-1.5-flash-latest',
                        'gemini-1.5-pro',         # Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø«Ø§Ù†ÙŠ: Ø°ÙƒÙŠ Ø¬Ø¯Ø§Ù‹
                        'gemini-1.5-pro-latest',
                        'gemini-1.0-pro'          # Ø§Ù„Ø®ÙŠØ§Ø± Ø§Ù„Ø¨Ø¯ÙŠÙ„
                    ]
                    
                    target_model = None
                    for model in preferred_models:
                        if model in all_models:
                            target_model = model
                            break
                    
                    if target_model:
                        self.model_name = target_model
                        logger.info(f"âœ… ØªÙ… Ø§Ø¹ØªÙ…Ø§Ø¯ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„: {self.model_name}")
                    else:
                        # Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø£ÙŠ Ù…ÙˆØ¯ÙŠÙ„ Ù…ÙØ¶Ù„ØŒ Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„ÙÙ„Ø§Ø´ Ø¥Ø¬Ø¨Ø§Ø±ÙŠØ§Ù‹
                        self.model_name = "gemini-1.5-flash"
                        logger.warning("âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…ÙˆØ¯ÙŠÙ„ Ù…ÙØ¶Ù„ØŒ ØªÙ… ÙØ±Ø¶ gemini-1.5-flash")
                        
                except Exception as e:
                    logger.warning(f"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§ÙƒØªØ´Ø§Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠØŒ Ø³Ù†Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ: {e}")
                    self.model_name = "gemini-1.5-flash"
            else:
                self.gemini_available = False
                logger.warning("âš ï¸ Google API Key missing")
            
            # 2. Ø¥Ø¹Ø¯Ø§Ø¯ OpenAI (Ù„Ù„ØµÙˆØ±)
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
            else:
                self.openai_available = False
            
            # 3. Ø¥Ø¹Ø¯Ø§Ø¯ Luma AI (Ù„Ù„ÙÙŠØ¯ÙŠÙˆ)
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            
            # 4. Ø¥Ø¹Ø¯Ø§Ø¯ Stability AI (Ø¨Ø¯ÙŠÙ„ Ù„Ù„ØµÙˆØ±)
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")

        except Exception as e:
            logger.error(f"âŒ API Setup Critical Error: {e}")
            self.gemini_available = False
            self.openai_available = False
            self.luma_available = False

    # ==================== Ø¯ÙˆØ§Ù„ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø­Ø¯ÙˆØ¯ (ÙƒØ§Ù…Ù„Ø©) ====================
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù‚Ø¯ ØªØ¬Ø§ÙˆØ² Ø§Ù„Ø­Ø¯ Ø§Ù„ÙŠÙˆÙ…ÙŠ"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒØ§Ø´ Ø£ÙˆÙ„Ø§Ù‹ Ù„ØªØ®ÙÙŠÙ Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute('SELECT usage_count FROM ai_usage WHERE user_id = ? AND service_type = ? AND usage_date = ?', (user_id, service_type, today))
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    self.user_limits_cache[cache_key] = current_usage
            
            # Ø¬Ù„Ø¨ Ø§Ù„Ø­Ø¯ÙˆØ¯ Ù…Ù† Ø§Ù„Ù…ØªØºÙŠØ±Ø§Øª
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            limit = limits_config.get(service_type, 20)
            
            if current_usage >= limit:
                return False, 0
            return True, limit - current_usage
            
        except Exception as e:
            logger.error(f"Limit Check Error: {e}")
            return True, 999 # Ø§Ù„Ø³Ù…Ø§Ø­ ÙÙŠ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø·Ø£

    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """ØªØ­Ø¯ÙŠØ« Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙƒØ§Ø´
            self.user_limits_cache[cache_key] = self.user_limits_cache.get(cache_key, 0) + 1
            
            # ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            with self.db.get_connection() as conn:
                conn.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                conn.commit()
            return True
        except Exception as e:
            logger.error(f"Usage Update Error: {e}")
            return False

    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© (Ù…Ø¹ Ø§Ù„Ø°Ø§ÙƒØ±Ø© ÙˆØ¥ØµÙ„Ø§Ø­ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„) ====================
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        try:
            # 1. ÙØ­Øµ Ø§Ù„Ø±ØµÙŠØ¯
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return "âŒ Ø¹Ø°Ø±Ø§Ù‹ØŒ Ù„Ù‚Ø¯ Ø§Ø³ØªÙ‡Ù„ÙƒØª Ø±ØµÙŠØ¯Ùƒ Ø§Ù„ÙŠÙˆÙ…ÙŠ Ù…Ù† Ø§Ù„Ø±Ø³Ø§Ø¦Ù„. Ø­Ø§ÙˆÙ„ ØºØ¯Ø§Ù‹."
            
            response_text = ""
            
            if use_gemini and self.gemini_available:
                # --- Ù…Ù†Ø·Ù‚ Ø§Ù„Ø°Ø§ÙƒØ±Ø© (Chat History) ---
                
                # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù‡Ù†Ø§Ùƒ Ø¬Ù„Ø³Ø© Ø³Ø§Ø¨Ù‚Ø© Ù„Ù‡Ø°Ø§ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ØŒ Ù†Ø¨Ø¯Ø£ ÙˆØ§Ø­Ø¯Ø© Ø¬Ø¯ÙŠØ¯Ø©
                if user_id not in self.chat_sessions:
                    try:
                        model = genai.GenerativeModel(self.model_name)
                        self.chat_sessions[user_id] = model.start_chat(history=[
                            {"role": "user", "parts": ["Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ØŒ ØªØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø¨Ø·Ù„Ø§Ù‚Ø©ØŒ ÙˆØªØªØ°ÙƒØ± Ø§Ø³Ù…ÙŠ ÙˆØ³ÙŠØ§Ù‚ Ø§Ù„ÙƒÙ„Ø§Ù…."]},
                            {"role": "model", "parts": ["Ø­Ø³Ù†Ø§Ù‹ØŒ ÙÙ‡Ù…Øª. Ø£Ù†Ø§ Ø¬Ø§Ù‡Ø² Ù„Ù„Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆØ³Ø£ØªØ°ÙƒØ± Ø³ÙŠØ§Ù‚ Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø©."]}
                        ])
                    except Exception as e:
                        logger.error(f"Start Chat Error: {e}")
                        # Ù…Ø­Ø§ÙˆÙ„Ø© Ø£Ø®ÙŠØ±Ø© Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„ÙÙ„Ø§Ø´ Ø§Ù„Ø¥Ø¬Ø¨Ø§Ø±ÙŠ
                        self.model_name = "gemini-1.5-flash"
                        model = genai.GenerativeModel(self.model_name)
                        self.chat_sessions[user_id] = model.start_chat(history=[])

                # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¬Ù„Ø³Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ©
                chat_session = self.chat_sessions[user_id]
                
                try:
                    # Ø¥Ø±Ø³Ø§Ù„ Ø§Ù„Ø±Ø³Ø§Ù„Ø© ÙˆØ§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ø±Ø¯
                    response = await chat_session.send_message_async(message)
                    response_text = response.text
                except Exception as e:
                    logger.warning(f"Session Error for {user_id}: {e}")
                    # ÙÙŠ Ø­Ø§Ù„Ø© Ø­Ø¯ÙˆØ« Ø®Ø·Ø£ (Ù…Ø«Ù„ Ø§Ù†ØªÙ‡Ø§Ø¡ Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ©)ØŒ Ù†Ø¹ÙŠØ¯ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ù„Ø³Ø©
                    try:
                        model = genai.GenerativeModel(self.model_name)
                        self.chat_sessions[user_id] = model.start_chat(history=[])
                        chat_session = self.chat_sessions[user_id]
                        response = await chat_session.send_message_async(message)
                        response_text = response.text
                    except Exception as final_e:
                         return f"âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„ Ø¨Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ {self.model_name}: {final_e}"

            elif self.openai_available:
                # OpenAI (fallback)
                try:
                    client = openai.OpenAI()
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": message}]
                    )
                    response_text = response.choices[0].message.content
                except Exception as e:
                    return f"âŒ Ø®Ø·Ø£ ÙÙŠ OpenAI: {e}"
            else:
                return "âŒ Ø®Ø¯Ù…Ø© Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ØºÙŠØ± Ù…ØªØ§Ø­Ø© Ø­Ø§Ù„ÙŠØ§Ù‹. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ù…ÙØ§ØªÙŠØ­ (GOOGLE_AI_API_KEY)."
            
            # ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© ÙˆØ­ÙØ¸ Ø§Ù„Ø±Ø¯
            self.update_user_usage(user_id, "ai_chat")
            self.db.save_ai_conversation(user_id, "chat", message, response_text)
            
            return response_text
            
        except Exception as e:
            logger.error(f"General Chat Error: {e}")
            return "âš ï¸ Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©."

    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ± (ÙƒØ§Ù…Ù„Ø©) ====================
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        try:
            allowed, _ = self.check_user_limit(user_id, "image_gen")
            if not allowed: return None, "âŒ Ø§Ù†ØªÙ‡Ù‰ Ø±ØµÙŠØ¯ Ø§Ù„ØµÙˆØ± Ø§Ù„ÙŠÙˆÙ…ÙŠ."
            
            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Gemini (Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„)
            enhanced_prompt = prompt
            if self.gemini_available:
                try:
                    # Ù†Ø³ØªØ®Ø¯Ù… Ù…ÙˆØ¯ÙŠÙ„ Ù…Ù†ÙØµÙ„ Ù„Ù„ØªØ­Ø³ÙŠÙ† Ø­ØªÙ‰ Ù„Ø§ Ù†Ø¤Ø«Ø± Ø¹Ù„Ù‰ Ø°Ø§ÙƒØ±Ø© Ø§Ù„Ø´Ø§Øª
                    model = genai.GenerativeModel(self.model_name)
                    resp = await model.generate_content_async(
                        f"Rewrite this prompt to be a detailed English description for DALL-E image generator, style: {style}. Prompt: {prompt}"
                    )
                    enhanced_prompt = resp.text
                except Exception as e:
                    logger.warning(f"Prompt enhancement failed: {e}")

            image_url = None
            
            # Ø§Ù„Ø®ÙŠØ§Ø± 1: OpenAI DALL-E 3
            if self.openai_available:
                try:
                    client = openai.OpenAI()
                    response = client.images.generate(
                        model="dall-e-3",
                        prompt=enhanced_prompt[:1000], # DALL-E limit
                        size="1024x1024",
                        quality="standard",
                        n=1
                    )
                    image_url = response.data[0].url
                except Exception as e:
                    logger.warning(f"DALL-E Error: {e}")

            # Ø§Ù„Ø®ÙŠØ§Ø± 2: Stability AI (Ø§Ù„Ø§Ø­ØªÙŠØ§Ø·ÙŠ)
            if not image_url and self.stability_api_key:
                try:
                    headers = {
                        "Authorization": f"Bearer {self.stability_api_key}",
                        "Content-Type": "application/json",
                        "Accept": "application/json"
                    }
                    data = {
                        "text_prompts": [{"text": enhanced_prompt, "weight": 1}],
                        "cfg_scale": 7,
                        "height": 512,
                        "width": 512,
                        "samples": 1
                    }
                    async with aiohttp.ClientSession() as session:
                        async with session.post(self.stable_diffusion_url, headers=headers, json=data) as resp:
                            if resp.status == 200:
                                # Ù…Ù„Ø§Ø­Ø¸Ø©: Stability ÙŠØ¹ÙŠØ¯ Ø§Ù„ØµÙˆØ±Ø© ÙƒÙ€ Base64ØŒ ÙˆÙ‡Ø°Ø§ ÙŠØ­ØªØ§Ø¬ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø®Ø§ØµØ© Ù„Ø±ÙØ¹Ù‡Ø§
                                # Ù„Ù„ØªØ¨Ø³ÙŠØ· Ù‡Ù†Ø§ØŒ Ø³Ù†Ø¹ÙŠØ¯ Ø±Ø³Ø§Ù„Ø© ØªÙˆØ¶ÙŠØ­ÙŠØ©
                                return None, "âš ï¸ ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­ (Stability)ØŒ Ù„ÙƒÙ† Ù†Ø¸Ø§Ù… Ø±ÙØ¹ Ø§Ù„Ù…Ù„ÙØ§Øª ÙŠØ­ØªØ§Ø¬ ØªÙ‡ÙŠØ¦Ø©."
                except Exception as e:
                    logger.warning(f"Stability Error: {e}")

            if image_url:
                self.update_user_usage(user_id, "image_gen")
                self.db.save_generated_file(user_id, "image", prompt, image_url)
                return image_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ù†Ø¬Ø§Ø­"
            
            return None, "âŒ ÙØ´Ù„ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØµÙˆØ±Ø©. ØªØ£ÙƒØ¯ Ù…Ù† ØªÙˆÙØ± Ø±ØµÙŠØ¯ ÙÙŠ OpenAI Ø£Ùˆ Stability."

        except Exception as e:
            logger.error(f"Image Gen Error: {e}")
            return None, "Ø­Ø¯Ø« Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ØµÙˆØ±."

    # ==================== Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (ÙƒØ§Ù…Ù„Ø©) ====================
    async def generate_video(self, user_id: int, prompt: str, image_url: str = None) -> Tuple[Optional[str], str]:
        try:
            allowed, _ = self.check_user_limit(user_id, "video_gen")
            if not allowed: return None, "âŒ Ø§Ù†ØªÙ‡Ù‰ Ø±ØµÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø§Ù„ÙŠÙˆÙ…ÙŠ."
            
            if not self.luma_available:
                return None, "âŒ Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ØºÙŠØ± Ù…ÙØ¹Ù„Ø© (LUMAAI_API_KEY ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯)."

            # ØªØ­Ø³ÙŠÙ† Ø§Ù„ÙˆØµÙ Ù„Ù„ÙÙŠØ¯ÙŠÙˆ
            enhanced_prompt = prompt
            if self.gemini_available:
                try:
                    model = genai.GenerativeModel(self.model_name)
                    resp = await model.generate_content_async(f"Enhance this video prompt, make it cinematic and detailed (English): {prompt}")
                    enhanced_prompt = resp.text
                except: pass

            # Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø·Ù„Ø¨ Ù„Ø®Ø¯Ù…Ø© Luma Dream Machine
            url = "https://api.lumalabs.ai/dream-machine/v1/generations"
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "prompt": enhanced_prompt,
                "aspect_ratio": "16:9"
            }
            
            if image_url:
                url = "https://api.lumalabs.ai/dream-machine/v1/generations/image"
                payload["image_url"] = image_url
            
            async with aiohttp.ClientSession() as session:
                # Ø¥Ø±Ø³Ø§Ù„ Ø·Ù„Ø¨ Ø§Ù„Ø¥Ù†Ø´Ø§Ø¡
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status != 201 and response.status != 200:
                        err_text = await response.text()
                        return None, f"âŒ Ø®Ø·Ø£ Ù…Ù† Luma: {response.status} - {err_text[:100]}"
                    
                    data = await response.json()
                    gen_id = data.get("id")
                    if not gen_id:
                        return None, "âŒ Ù„Ù… ÙŠØªÙ… Ø§Ø³ØªÙ„Ø§Ù… Ù…Ø¹Ø±Ù Ø§Ù„ÙÙŠØ¯ÙŠÙˆ."
                    
                    # Ø§Ù„Ø§Ù†ØªØ¸Ø§Ø± Ø­ØªÙ‰ ÙŠÙƒØªÙ…Ù„ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ (Polling)
                    # Ù†Ù†ØªØ¸Ø± Ø¨Ø­Ø¯ Ø£Ù‚ØµÙ‰ 60 Ù…Ø­Ø§ÙˆÙ„Ø© * 5 Ø«ÙˆØ§Ù†ÙŠ = 5 Ø¯Ù‚Ø§Ø¦Ù‚
                    for _ in range(60):
                        await asyncio.sleep(5)
                        async with session.get(f"{url}/{gen_id}", headers=headers) as check_resp:
                            if check_resp.status == 200:
                                status_data = await check_resp.json()
                                state = status_data.get("state")
                                
                                if state == "completed":
                                    video_url = status_data.get("assets", {}).get("video")
                                    if video_url:
                                        self.update_user_usage(user_id, "video_gen")
                                        self.db.save_generated_file(user_id, "video", prompt, video_url)
                                        return video_url, "âœ… ØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ Ø¨Ù†Ø¬Ø§Ø­!"
                                elif state == "failed":
                                    return None, f"âŒ ÙØ´Ù„ ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ: {status_data.get('failure_reason')}"
            
            return None, "âš ï¸ Ø§Ø³ØªØºØ±Ù‚ Ø§Ù„ÙÙŠØ¯ÙŠÙˆ ÙˆÙ‚ØªØ§Ù‹ Ø·ÙˆÙŠÙ„Ø§Ù‹ Ø¬Ø¯Ø§Ù‹ØŒ Ø³ÙŠØªÙ… Ø¥Ø´Ø¹Ø§Ø±Ùƒ Ø¹Ù†Ø¯ Ø§ÙƒØªÙ…Ø§Ù„Ù‡."

        except Exception as e:
            logger.error(f"Video Error: {e}")
            return None, "Ø­Ø¯Ø« Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ ÙÙŠ Ø®Ø¯Ù…Ø© Ø§Ù„ÙÙŠØ¯ÙŠÙˆ."

    # ==================== Ø¯ÙˆØ§Ù„ Ù…Ø³Ø§Ø¹Ø¯Ø© ÙˆÙ…Ø¹Ù„ÙˆÙ…Ø§Øª ====================
    def get_available_services(self) -> Dict[str, bool]:
        """Ø¥Ø±Ø¬Ø§Ø¹ Ø­Ø§Ù„Ø© Ø§Ù„Ø®Ø¯Ù…Ø§Øª Ø§Ù„Ù…ØªØ§Ø­Ø©"""
        return {
            "chat": self.gemini_available or self.openai_available,
            "image_generation": self.openai_available or bool(self.stability_api_key),
            "video_generation": self.luma_available
        }
        
    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        """Ø¥Ø±Ø¬Ø§Ø¹ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… Ù„Ù„ÙŠÙˆÙ…"""
        stats = {}
        for s_type in ["ai_chat", "image_gen", "video_gen"]:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{s_type}"
            stats[s_type] = self.user_limits_cache.get(cache_key, 0)
        return stats
