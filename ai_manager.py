# ai_manager.py - ุงูุฅุตุฏุงุฑ ุงูุนููุงู (ูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู + ูุงูุฉ ุงูุฎุฏูุงุช + ุชูุซูู ูุงูู)
# ูุฐุง ุงูููู ูุญุชูู ุนูู ูุงูุฉ ุงูุชุนุฏููุงุช ุงููุทููุจุฉ ูุน ุงูุญูุงุธ ุนูู ุงูุจููุฉ ุงูุฃุณุงุณูุฉ
# ุงููููุฒุงุช ุงูุฌุฏูุฏุฉ:
# 1. ูุธุงู ุฏูุงุน ุซูุงุซู ููููุฏููุงุช (Gemini 2.5 -> 2.0 -> 1.5)
# 2. ุชูุธูู ุงูุฑุฏูุฏ ูู ุงููุตูุต ุงูุชุญููููุฉ (THOUGHT)
# 3. ุฏุนู ูุงูู ูููุณุน ูุฎุฏูุงุช ุงูุตูุฑ ูุงูููุฏูู
# 4. ูุนุงูุฌุฉ ุฏูููุฉ ููุฃุฎุทุงุก ูุงูุญุฏูุฏ

import os
import logging
import asyncio
import google.generativeai as genai
import openai
import aiohttp
import re  # ููุชุจุฉ ุงูุชุนุงูู ูุน ุงููุตูุต (Regex) ูุชูุธูู ุงูุฑุฏูุฏ
from datetime import datetime
from typing import Optional, List, Dict, Any, Tuple

# ุฅุนุฏุงุฏ ูุธุงู ุงูุชุณุฌูู (Logging) ููุชุงุจุนุฉ ุงูุฃุฎุทุงุก ุจุฏูุฉ
logger = logging.getLogger(__name__)

class AIManager:
    """
    ูุฏูุฑ ุฎุฏูุงุช ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุชูุงูู.
    
    ูููู ูุฐุง ุงูููุงุณ ุจุฅุฏุงุฑุฉ:
    1. ูุญุงุฏุซุงุช Gemini ูุน ูุธุงู ุฐุงูุฑุฉ ููุธุงู ุทูุงุฑุฆ ูุชุทูุฑ.
    2. ุชูููุฏ ุงูุตูุฑ ุจุงุณุชุฎุฏุงู OpenAI DALL-E 3 ุฃู Stability AI.
    3. ุชูููุฏ ุงูููุฏูู ุจุงุณุชุฎุฏุงู Luma Dream Machine.
    4. ุชุชุจุน ุงุณุชููุงู ุงููุณุชุฎุฏููู ูุงูุญุฏูุฏ ุงูููููุฉ.
    """
    
    def __init__(self, db):
        """
        ุชููุฆุฉ ูุฏูุฑ ุงูุฐูุงุก ุงูุงุตุทูุงุนู.
        :param db: ูุงุฆู ูุงุนุฏุฉ ุงูุจูุงูุงุช ููุชุนุงูู ูุน ุงูุชุฎุฒูู.
        """
        self.db = db
        
        # ูุงููุณ ูุชุฎุฒูู ุฌูุณุงุช ุงููุญุงุฏุซุฉ ุงููุดุทุฉ (Chat Sessions)
        # ุงูููุชุงุญ: user_idุ ุงููููุฉ: ูุงุฆู ChatSession
        self.chat_sessions: Dict[int, genai.ChatSession] = {} 
        
        # ุงูููุฏูู ุงูุงูุชุฑุงุถู ุงููุจุฏุฆู (ุณูุชู ุชุญุฏูุซู ุชููุงุฆูุงู ูู setup_apis)
        self.model_name = "gemini-2.5-flash" 
        
        # ูุงุด ูุญูู ูุชุฎุฒูู ุญุฏูุฏ ุงูุงุณุชุฎุฏุงู ูุชูููู ุงูุถุบุท ุนูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
        self.user_limits_cache = {}
        
        # ุงุณุชุฏุนุงุก ุฏุงูุฉ ุฅุนุฏุงุฏ ูุงุฌูุงุช ุงูุจุฑูุฌุฉ
        self.setup_apis()
        
    def setup_apis(self):
        """
        ุฅุนุฏุงุฏ ููุงุชูุญ ูุงุฌูุงุช ุจุฑูุฌุฉ ุงูุชุทุจููุงุช (APIs) ูุงุฎุชูุงุฑ ุงูููุฏูู ุงูุฃูุณุจ.
        ูุชู ุงูุชุญูู ูู ูุฌูุฏ ุงูููุงุชูุญ ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ ูุชููุฆุฉ ุงูููุชุจุงุช.
        """
        try:
            # ==================== 1. ุฅุนุฏุงุฏ Google Gemini ====================
            google_api_key = os.getenv("GOOGLE_AI_API_KEY")
            if google_api_key:
                genai.configure(api_key=google_api_key)
                self.gemini_available = True
                logger.info("โ ุชู ุงูุงุชุตุงู ุจุฎุฏูุฉ Google Gemini ุจูุฌุงุญ.")
                
                # --- ุงูุชุดุงู ุงูููุฏููุงุช ุงููุชุงุญุฉ ูุชุฑุชูุจ ุงูุฃููููุงุช ---
                try:
                    logger.info("๐ ุฌุงุฑู ูุญุต ููุฏููุงุช Gemini ุงููุชุงุญุฉ ูู ุงูุญุณุงุจ...")
                    
                    # ุงุณุชุฎุฑุงุฌ ูุงุฆูุฉ ุงูููุฏููุงุช ุงูุชู ุชุฏุนู ุชูููุฏ ุงููุญุชูู
                    all_models = []
                    for m in genai.list_models():
                        if 'generateContent' in m.supported_generation_methods:
                            model_name = m.name.replace('models/', '')
                            all_models.append(model_name)
                            
                    logger.info(f"๐ ุงูููุฏููุงุช ุงูููุชุดูุฉ: {all_models}")
                    
                    # ุงููุงุฆูุฉ ุงูููุถูุฉ (ุงูุชุฑุชูุจ ุงูุชูุงุฒูู ุญุณุจ ุงูููุฉ ูุงูุญุฏุงุซุฉ)
                    # ูุฐุง ุงูุชุฑุชูุจ ูุญุฏุฏ ูู ุณูููู "ุงูููุงุฌู ุงูุฃุณุงุณู"
                    preferred_models = [
                        'gemini-2.5-flash',       # ุงูุฃุณุงุณู: ุงูุฃููู ูุงูุฃุญุฏุซ
                        'gemini-2.0-flash',       # ุงูุฏูุงุน ุงูุฃูู: ุชูุงุฒู ููุชุงุฒ ุจูู ุงูุณุฑุนุฉ ูุงูุฐูุงุก
                        'gemini-1.5-pro-latest',  # ุฎูุงุฑ ููู ุฌุฏุงู
                        'gemini-1.5-pro',
                        'gemini-1.5-flash'        # ุงูุฏูุงุน ุงูุฃุฎูุฑ: ุงูุฃูุซุฑ ุงุณุชูุฑุงุฑุงู
                    ]
                    
                    target_model = None
                    for model in preferred_models:
                        if model in all_models:
                            target_model = model
                            break
                    
                    if target_model:
                        self.model_name = target_model
                        logger.info(f"โ ุชู ุงุนุชูุงุฏ ุงูููุฏูู ุงูุฃุณุงุณู: {self.model_name}")
                    else:
                        # ุฅุฐุง ูู ูุฌุฏ ุฃู ููุฏูู ูุนุฑููุ ููุชุฑุถ ูุฌูุฏ 2.5 ูุฎูุงุฑ ุงูุชุฑุงุถู
                        self.model_name = "gemini-2.5-flash"
                        logger.warning("โ๏ธ ูู ูุชู ุงูุนุซูุฑ ุนูู ููุฏูู ููุถู ูู ุงููุงุฆูุฉุ ุชู ูุฑุถ gemini-2.5-flash")
                        
                except Exception as e:
                    logger.warning(f"โ๏ธ ุญุฏุซ ุฎุทุฃ ุฃุซูุงุก ุงูุงูุชุดุงู ุงูุชููุงุฆู ููููุฏููุงุช: {e}")
                    # ูู ุญุงูุฉ ุงูุฎุทุฃุ ูุนูุฏ ููููุฏูู ุงูููู ูุงูุชุฑุงุถู
                    self.model_name = "gemini-2.5-flash"
            else:
                self.gemini_available = False
                logger.warning("โ๏ธ ููุชุงุญ Google API ุบูุฑ ููุฌูุฏ (GOOGLE_AI_API_KEY).")
            
            # ==================== 2. ุฅุนุฏุงุฏ OpenAI (ููุตูุฑ) ====================
            openai_api_key = os.getenv("OPENAI_API_KEY")
            if openai_api_key:
                openai.api_key = openai_api_key
                self.openai_available = True
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ OpenAI.")
            else:
                self.openai_available = False
                logger.info("โน๏ธ ุฎุฏูุฉ OpenAI ุบูุฑ ููุนูุฉ.")
            
            # ==================== 3. ุฅุนุฏุงุฏ Luma AI (ููููุฏูู) ====================
            self.luma_api_key = os.getenv("LUMAAI_API_KEY")
            self.luma_available = bool(self.luma_api_key)
            if self.luma_available:
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ Luma AI ููููุฏูู.")
            
            # ==================== 4. ุฅุนุฏุงุฏ Stability AI (ุจุฏูู ููุตูุฑ) ====================
            self.stability_api_key = os.getenv("STABILITY_API_KEY")
            self.stable_diffusion_url = os.getenv("STABLE_DIFFUSION_URL", "https://api.stability.ai/v1/generation/stable-diffusion-v1-6/text-to-image")
            if self.stability_api_key:
                logger.info("โ ุชู ุชูุนูู ุฎุฏูุฉ Stability AI.")

        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ุญุฑุฌ ูู ุฅุนุฏุงุฏ ุงููุงุฌูุงุช (Setup Error): {e}")
            self.gemini_available = False
            self.openai_available = False
            self.luma_available = False
  
    # ==================== ุฏุงูุฉ ุชูุธูู ุงูุฑุฏูุฏ (Clean Response) ====================
    def clean_response(self, text: str) -> str:
        """
        ุชููู ูุฐู ุงูุฏุงูุฉ ุจุชูุธูู ุฑุฏ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู "ุฃููุงุฑู ุงูุฏุงุฎููุฉ".
        ุงูููุฏููุงุช ุงูุฌุฏูุฏุฉ (ูุซู 2.5) ูุฏ ุชุทุจุน ุฎุทูุงุช ุงูุชูููุฑ (Thinking Process) ูุจู ุงูุฑุฏ.
        ูุฐู ุงูุฏุงูุฉ ุชุฒูู ุชูู ุงูุฃููุงุฑ ูุชุนุทู ุงููุณุชุฎุฏู ุงูุฑุฏ ุงูููุงุฆู ููุท.
        
        :param text: ุงููุต ุงูุฎุงู ุงููุงุฏู ูู ุงูููุฏูู.
        :return: ุงููุต ุงููุธูู ุงูุฌุงูุฒ ููุฅุฑุณุงู.
        """
        if not text:
            return "ุนุฐุฑุงูุ ูู ุฃุณุชุทุน ุชูููู ุฑุฏ ููุงุณุจ ูู ุงูููุช ุงูุญุงูู."
        
        # ุญูุธ ุงููุต ุงูุฃุตูู ูุงุณุชุฎุฏุงูู ูู ุญุงูุฉ ุงูุทูุงุฑุฆ (ุฅุฐุง ูุณุญูุง ูู ุดูุก ุจุงูุฎุทุฃ)
        original_text = text
        
        # ููุท Regex ููุจุญุซ ุนู ูุชู ุงูุชูููุฑ ูุญุฐููุง
        # ูุจุญุซ ุนู ูููุฉ THOUGHT: ููุญุฐู ูู ุดูุก ุจุนุฏูุง ุญุชู ูุฌุฏ ุณุทุฑูู ูุงุฑุบูู ุฃู ููุงูุฉ ุงููุต
        # FLAGS: DOTALL (ุงูููุทุฉ ุชุดูู ุงูุฃุณุทุฑ ุงูุฌุฏูุฏุฉ) | IGNORECASE (ุชุฌุงูู ุญุงูุฉ ุงูุฃุญุฑู)
        clean_text = re.sub(r'THOUGHT:.*?(?=\n\n|\Z)', '', text, flags=re.DOTALL | re.IGNORECASE)
        
        # ุชูุธูู ุฅุถุงูู ูุฃู ุจูุงูุง (ูุซู ูุณุงูุงุช ุฒุงุฆุฏุฉ ุฃู ูููุฉ THOUGHT ูุชุจููุฉ)
        clean_text = clean_text.replace("THOUGHT:", "").strip()
        
        # ๐ฅ ุญูุงูุฉ ูู ุงูุฑุณุงูุฉ ุงููุงุฑุบุฉ ๐ฅ
        # ุฅุฐุง ูุงู ุงูุชูุธูู ูุฏ ูุณุญ ูู ุดูุก (ูุซูุงู ุงูููุฏูู ุฃุฑุณู ุชูููุฑุงู ููุท ุจุฏูู ุฑุฏ)ุ
        # ูุฑุฌุน ุงููุต ุงูุฃุตูู ููุง ูู ููู ูุง ูุธูุฑ ูููุณุชุฎุฏู ุฑุณุงูุฉ ูุงุฑุบุฉ.
        if not clean_text or len(clean_text) < 2:
            return original_text
            
        return clean_text

    # ==================== ุฏูุงู ุฅุฏุงุฑุฉ ุงูุญุฏูุฏ (Limits Management) ====================
    def check_user_limit(self, user_id: int, service_type: str = "ai_chat") -> Tuple[bool, int]:
        """
        ุงูุชุญูู ููุง ุฅุฐุง ูุงู ุงููุณุชุฎุฏู ูุฏ ุชุฌุงูุฒ ุงูุญุฏ ุงููููู ุงููุณููุญ ุจู ููุฎุฏูุฉ.
        
        :param user_id: ูุนุฑู ุงููุณุชุฎุฏู.
        :param service_type: ููุน ุงูุฎุฏูุฉ (ai_chat, image_gen, video_gen).
        :return: (ูุณููุญ_ุฃู_ูุง, ุงููุชุจูู).
        """
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # 1. ุงูุชุญูู ูู ุงููุงุด ุฃููุงู (ุฃุณุฑุน)
            if cache_key in self.user_limits_cache:
                current_usage = self.user_limits_cache[cache_key]
            else:
                # 2. ุงูุชุญูู ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช ุฅุฐุง ูู ููู ูู ุงููุงุด
                with self.db.get_connection() as conn:
                    cursor = conn.cursor()
                    cursor.execute(
                        'SELECT usage_count FROM ai_usage WHERE user_id = ? AND service_type = ? AND usage_date = ?', 
                        (user_id, service_type, today)
                    )
                    result = cursor.fetchone()
                    current_usage = result[0] if result else 0
                    # ุชุญุฏูุซ ุงููุงุด
                    self.user_limits_cache[cache_key] = current_usage
            
            # ุฌูุจ ุงูุญุฏูุฏ ูู ูุชุบูุฑุงุช ุงูุจูุฆุฉ (ุฃู ุงุณุชุฎุฏุงู ุงูููู ุงูุงูุชุฑุงุถูุฉ)
            limits_config = {
                "ai_chat": int(os.getenv("DAILY_AI_LIMIT", "20")),
                "image_gen": int(os.getenv("DAILY_IMAGE_LIMIT", "5")),
                "video_gen": int(os.getenv("DAILY_VIDEO_LIMIT", "2"))
            }
            
            limit = limits_config.get(service_type, 20)
            
            # ุงูุชุญูู ุงูููุงุฆู
            if current_usage >= limit:
                return False, 0
            
            return True, limit - current_usage
            
        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ูู ูุญุต ุงูุญุฏูุฏ (Limit Check Error): {e}")
            # ูู ุญุงูุฉ ุงูุฎุทุฃุ ูุณูุญ ุจุงูุนูููุฉ ูุชุฌูุจ ุชุนุทูู ุงููุณุชุฎุฏู
            return True, 999 

    def update_user_usage(self, user_id: int, service_type: str = "ai_chat") -> bool:
        """
        ุชุญุฏูุซ ุณุฌู ุงุณุชููุงู ุงููุณุชุฎุฏู ุจุนุฏ ูุฌุงุญ ุงูุนูููุฉ.
        
        :param user_id: ูุนุฑู ุงููุณุชุฎุฏู.
        :param service_type: ููุน ุงูุฎุฏูุฉ.
        :return: True ุฅุฐุง ุชู ุงูุชุญุฏูุซ ุจูุฌุงุญ.
        """
        try:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{service_type}"
            
            # ุชุญุฏูุซ ุงููุงุด
            self.user_limits_cache[cache_key] = self.user_limits_cache.get(cache_key, 0) + 1
            
            # ุชุญุฏูุซ ูุงุนุฏุฉ ุงูุจูุงูุงุช (Insert or Update)
            with self.db.get_connection() as conn:
                conn.execute('''
                INSERT INTO ai_usage (user_id, service_type, usage_date, usage_count)
                VALUES (?, ?, ?, 1)
                ON CONFLICT(user_id, service_type, usage_date) 
                DO UPDATE SET usage_count = usage_count + 1
                ''', (user_id, service_type, today))
                conn.commit()
            return True
        except Exception as e:
            logger.error(f"โ ุฎุทุฃ ูู ุชุญุฏูุซ ุงูุงุณุชููุงู (Usage Update Error): {e}")
            return False

    # ==================== ุฎุฏูุฉ ุงููุญุงุฏุซุฉ (ูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู) ====================
    async def chat_with_ai(self, user_id: int, message: str, use_gemini: bool = True) -> str:
        """
        ุฅุฌุฑุงุก ูุญุงุฏุซุฉ ุฐููุฉ ูุน ุงููุณุชุฎุฏู ุจุงุณุชุฎุฏุงู ุงุณุชุฑุงุชูุฌูุฉ ุงูุฏูุงุน ุงูุซูุงุซู.
        
        ุงูุงุณุชุฑุงุชูุฌูุฉ:
        1. ุงููุญุงููุฉ ุจู Gemini 2.5 Flash (ุงูุฃุฐูู).
        2. ุนูุฏ ุงููุดูุ ุงููุญุงููุฉ ุจู Gemini 2.0 Flash (ุงููุชูุงุฒู).
        3. ุนูุฏ ุงููุดูุ ุงููุญุงููุฉ ุจู Gemini 1.5 Flash (ุงููููุฐ - ุญุฏูุฏ ุนุงููุฉ).
        """
        try:
            # 1. ุงูุชุญูู ูู ุฑุตูุฏ ุงููุณุชุฎุฏู
            allowed, remaining = self.check_user_limit(user_id, "ai_chat")
            if not allowed:
                return "โ ุนุฐุฑุงูุ ููุฏ ุงุณุชูููุช ุฑุตูุฏู ุงููููู ูู ุงูุฑุณุงุฆู. ูุชุฌุฏุฏ ุงูุฑุตูุฏ ุบุฏุงู."
            
            response_text = ""
            
            if use_gemini and self.gemini_available:
                
                # -----------------------------------------------------------
                # ๐ข ุงููุญุงููุฉ ุงูุฃููู: ุงูููุฏูู ุงูุฃุณุงุณู (Gemini 2.5 Flash)
                # -----------------------------------------------------------
                try:
                    # ุฅุนุฏุงุฏ ุงูุฌูุณุฉ (Memory) ุฅุฐุง ูู ุชูู ููุฌูุฏุฉ
                    if user_id not in self.chat_sessions:
                        # ูุณุชุฎุฏู ุงูููุฏูู 2.5 ูุฅูุดุงุก ุงูุฌูุณุฉ
                        model_v1 = genai.GenerativeModel("gemini-2.5-flash")
                        self.chat_sessions[user_id] = model_v1.start_chat(history=[
                            {"role": "user", "parts": ["ุฃูุช ูุณุงุนุฏ ุฐูู ููููุฏ ุชุชุญุฏุซ ุงูุนุฑุจูุฉ ุจุทูุงูุฉ. ุฑุฏ ูุจุงุดุฑุฉ ุฏูู ููุฏูุงุช ุทูููุฉ."]},
                            {"role": "model", "parts": ["ุญุณูุงูุ ูููุช. ุณุฃุฑุฏ ูุจุงุดุฑุฉ ูุจุงููุบุฉ ุงูุนุฑุจูุฉ."]}
                        ])
                    
                    # ูุญุงููุฉ ุฅุฑุณุงู ุงูุฑุณุงูุฉ
                    chat_session = self.chat_sessions[user_id]
                    response = await chat_session.send_message_async(message)
                    
                    # ุชูุธูู ุงูุฑุฏ
                    response_text = self.clean_response(response.text)
                    
                except Exception as e1:
                    # ุชุณุฌูู ูุดู ุงูููุฏูู ุงูุฃูู
                    logger.warning(f"โ๏ธ ูุดู ุงูููุฏูู ุงูุฃุณุงุณู (2.5-flash): {e1}. ุฌุงุฑู ุงูุงูุชูุงู ูุฎุท ุงูุฏูุงุน ุงูุฃูู...")
                    
                    # -----------------------------------------------------------
                    # ๐ก ุงููุญุงููุฉ ุงูุซุงููุฉ: ุฎุท ุงูุฏูุงุน ุงูุฃูู (Gemini 2.0 Flash)
                    # -----------------------------------------------------------
                    try:
                        # ูุณุชุฎุฏู generate_content ูุถูุงู ุณุฑุนุฉ ุงูุฑุฏ ูุชุฌุงูุฒ ูุดุงูู ุงูุฌูุณุฉ ุงููุนููุฉ
                        fallback_model_1 = genai.GenerativeModel("gemini-2.0-flash")
                        response = await fallback_model_1.generate_content_async(message)
                        
                        # ุชูุธูู ุงูุฑุฏ
                        response_text = self.clean_response(response.text)
                        
                    except Exception as e2:
                        # ุชุณุฌูู ูุดู ุงูููุฏูู ุงูุซุงูู
                        logger.warning(f"โ๏ธ ูุดู ุฎุท ุงูุฏูุงุน ุงูุฃูู (2.0-flash): {e2}. ุฌุงุฑู ุงูุงูุชูุงู ูุฎุท ุงูุฏูุงุน ุงูุฃุฎูุฑ...")
                        
                        # -----------------------------------------------------------
                        # ๐ด ุงููุญุงููุฉ ุงูุซุงูุซุฉ: ุฎุท ุงูุฏูุงุน ุงูุฃุฎูุฑ (Gemini 1.5 Flash)
                        # ูุฐุง ุงูููุฏูู ูุชููุฒ ุจุญุฏูุฏ ุงุณุชุฎุฏุงู ุนุงููุฉ ุฌุฏุงู (High Quota)
                        # -----------------------------------------------------------
                        try:
                            fallback_model_2 = genai.GenerativeModel("gemini-1.5-flash")
                            response = await fallback_model_2.generate_content_async(message)
                            
                            # ุชูุธูู ุงูุฑุฏ
                            response_text = self.clean_response(response.text)
                            
                        except Exception as e3:
                            # ุฅุฐุง ูุดูุช ุงูุซูุงุซ ูุญุงููุงุชุ ููุฐุง ูุนูู ูุฌูุฏ ูุดููุฉ ุนุงูุฉ ูู ุฎูุงุฏู ุฌูุฌู
                            logger.error(f"โ ูุดู ุฌููุน ุงูููุฏููุงุช (2.5 -> 2.0 -> 1.5): {e3}")
                            return "โ๏ธ ุฌููุน ุฎูุงุฏู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูุดุบููุฉ ุฌุฏุงู ุญุงููุงูุ ูุฑุฌู ุงูุงูุชุธุงุฑ ุฏูููุฉ ูุงููุญุงููุฉ ูุฌุฏุฏุงู."

            elif self.openai_available:
                # ุฎูุงุฑ OpenAI ุฅุฐุง ูุงู ููุนูุงู ูุจุฏูู ููู
                try:
                    client = openai.OpenAI()
                    response = client.chat.completions.create(
                        model="gpt-4o-mini",
                        messages=[{"role": "user", "content": message}]
                    )
                    response_text = response.choices[0].message.content
                except Exception as e:
                    logger.error(f"โ ุฎุทุฃ ูู OpenAI Chat: {e}")
                    return f"โ ุญุฏุซ ุฎุทุฃ ูู ุฎุฏูุฉ OpenAI: {e}"
            else:
                return "โ ุฎุฏูุฉ ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุบูุฑ ูุชุงุญุฉ ุญุงููุงู. ูุฑุฌู ุงูุชุฃูุฏ ูู ููุงุชูุญ API."
            
            # ุชุณุฌูู ุงูุนูููุฉ ูุญูุธ ุงูุฑุฏ ูู ูุงุนุฏุฉ ุงูุจูุงูุงุช
            self.update_user_usage(user_id, "ai_chat")
            self.db.save_ai_conversation(user_id, "chat", message, response_text)
            
            return response_text
            
        except Exception as e:
            logger.error(f"โ General Chat Error (ุฎุทุฃ ุนุงู): {e}")
            return "โ๏ธ ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ุฃุซูุงุก ุงููุนุงูุฌุฉ."

    # ==================== ุฎุฏูุฉ ุงูุตูุฑ (ูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู ูููุตู) ====================
    async def generate_image(self, user_id: int, prompt: str, style: str = "realistic") -> Tuple[Optional[str], str]:
        """
        ุชูููุฏ ุตูุฑุฉ ุจูุงุกู ุนูู ุงููุตู ุงููุตู.
        ูุชู ุชุญุณูู ุงููุตู ุฃููุงู ุจุงุณุชุฎุฏุงู Gemini (ุจูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู)ุ ุซู ุฅุฑุณุงูู ูู DALL-E.
        """
        try:
            # 1. ุงูุชุญูู ูู ุงูุญุฏูุฏ
            allowed, _ = self.check_user_limit(user_id, "image_gen")
            if not allowed: return None, "โ ุงูุชูู ุฑุตูุฏ ุงูุตูุฑ ุงููููู."
            
            # 2. ุชุญุณูู ุงููุตู (Prompt Engineering) ุจุงุณุชุฎุฏุงู Gemini
            # ุณูุญุงูู ุจู 3 ููุฏููุงุช ูุถูุงู ุงูุญุตูู ุนูู ูุตู ูุญุณู
            enhanced_prompt = prompt
            
            if self.gemini_available:
                # ูุญุงููุฉ 1: Gemini 2.5
                try:
                    m = genai.GenerativeModel("gemini-2.5-flash")
                    r = await m.generate_content_async(f"Rewrite this prompt to be a detailed English description for DALL-E image generator, style: {style}. Prompt: {prompt}")
                    enhanced_prompt = self.clean_response(r.text)
                except:
                    # ูุญุงููุฉ 2: Gemini 2.0
                    try:
                        m = genai.GenerativeModel("gemini-2.0-flash")
                        r = await m.generate_content_async(f"Rewrite prompt for DALL-E image generation: {prompt}")
                        enhanced_prompt = self.clean_response(r.text)
                    except:
                        # ูุญุงููุฉ 3: Gemini 1.5
                        try:
                            m = genai.GenerativeModel("gemini-1.5-flash")
                            r = await m.generate_content_async(f"English prompt for DALL-E: {prompt}")
                            enhanced_prompt = self.clean_response(r.text)
                        except:
                            # ุฅุฐุง ูุดู ุงูุฌููุนุ ูุณุชุฎุฏู ุงููุตู ุงูุฃุตูู ููุง ูู
                            logger.warning("โ๏ธ ูุดู ุชุญุณูู ูุตู ุงูุตูุฑุฉ ุจุฌููุน ุงูููุฏููุงุชุ ุงุณุชุฎุฏุงู ุงููุตู ุงูุฃุตูู.")
                            pass

            image_url = None
            
            # 3. ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู OpenAI DALL-E 3 (ุงูุฎูุงุฑ ุงูุฃูุถู)
            if self.openai_available:
                try:
                    client = openai.OpenAI()
                    response = client.images.generate(
                        model="dall-e-3",
                        prompt=enhanced_prompt[:1000],  # DALL-E ููุจู 1000 ุญุฑู ูุญุฏ ุฃูุตู
                        size="1024x1024",
                        quality="standard",
                        n=1
                    )
                    image_url = response.data[0].url
                except Exception as e:
                    logger.warning(f"โ ุฎุทุฃ DALL-E: {e}")

            # 4. ุงูุชูููุฏ ุจุงุณุชุฎุฏุงู Stability AI (ุงูุฎูุงุฑ ุงูุงุญุชูุงุทู)
            if not image_url and self.stability_api_key:
                try:
                    headers = {
                        "Authorization": f"Bearer {self.stability_api_key}",
                        "Content-Type": "application/json",
                        "Accept": "application/json"
                    }
                    data = {
                        "text_prompts": [{"text": enhanced_prompt, "weight": 1}],
                        "cfg_scale": 7,
                        "height": 512,
                        "width": 512,
                        "samples": 1
                    }
                    async with aiohttp.ClientSession() as session:
                        async with session.post(self.stable_diffusion_url, headers=headers, json=data) as resp:
                            if resp.status == 200:
                                # ููุงุญุธุฉ: Stability ูุนูุฏ ุงูุตูุฑุฉ ูู Base64ุ ููุฐุง ูุญุชุงุฌ ููุฏ ุฅุถุงูู ููุฑูุน
                                # ููุง ููุชูู ุจุฅุดุนุงุฑ ุงููุฌุงุญ ูุชุฌูุจ ุชุนููุฏ ุงูููุฏ ุฃูุซุฑ ูู ุงููุงุฒู
                                return None, "โ๏ธ ุชู ุฅูุดุงุก ุงูุตูุฑุฉ ุจูุฌุงุญ (Stability)ุ ููู ูุชุทูุจ ุณูุฑูุฑ ูุฑูุนูุง."
                            else:
                                logger.error(f"Stability Error: {await resp.text()}")
                except Exception as e:
                    logger.warning(f"โ ุฎุทุฃ Stability: {e}")

            # 5. ุงููุชูุฌุฉ ุงูููุงุฆูุฉ
            if image_url:
                self.update_user_usage(user_id, "image_gen")
                self.db.save_generated_file(user_id, "image", prompt, image_url)
                return image_url, "โ ุชู ุฅูุดุงุก ุงูุตูุฑุฉ ุจูุฌุงุญ"
            
            return None, "โ ูุดู ุฅูุดุงุก ุงูุตูุฑุฉ. ุชุฃูุฏ ูู ุชููุฑ ุฑุตูุฏ ูู OpenAI."

        except Exception as e:
            logger.error(f"โ Image Gen Error: {e}")
            return None, "ุญุฏุซ ุฎุทุฃ ุบูุฑ ูุชููุน ูู ุฎุฏูุฉ ุงูุตูุฑ."

    # ==================== ุฎุฏูุฉ ุงูููุฏูู (ูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู ูููุตู) ====================
    async def generate_video(self, user_id: int, prompt: str, image_url: str = None) -> Tuple[Optional[str], str]:
        """
        ุชูููุฏ ููุฏูู ุจุงุณุชุฎุฏุงู Luma Dream Machine.
        ูุชุถูู ุชุญุณูู ุงููุตู ุฃููุงู ุจูุธุงู ุงูุฏูุงุน ุงูุซูุงุซู.
        """
        try:
            # 1. ุงูุชุญูู ูู ุงูุญุฏูุฏ
            allowed, _ = self.check_user_limit(user_id, "video_gen")
            if not allowed: return None, "โ ุงูุชูู ุฑุตูุฏ ุงูููุฏูู ุงููููู."
            
            if not self.luma_available:
                return None, "โ ุฎุฏูุฉ ุงูููุฏูู ุบูุฑ ููุนูุฉ (LUMAAI_API_KEY ุบูุฑ ููุฌูุฏ)."

            # 2. ุชุญุณูู ุงููุตู ููููุฏูู (ุซูุงุซ ูุญุงููุงุช)
            enhanced_prompt = prompt
            if self.gemini_available:
                # ูุญุงููุฉ 1: 2.5
                try:
                    m = genai.GenerativeModel("gemini-2.5-flash")
                    r = await m.generate_content_async(f"Enhance this video prompt, make it cinematic and detailed (English): {prompt}")
                    enhanced_prompt = self.clean_response(r.text)
                except:
                    # ูุญุงููุฉ 2: 2.0
                    try:
                        m = genai.GenerativeModel("gemini-2.0-flash")
                        r = await m.generate_content_async(f"Enhance video prompt (English): {prompt}")
                        enhanced_prompt = self.clean_response(r.text)
                    except:
                        # ูุญุงููุฉ 3: 1.5
                        try:
                            m = genai.GenerativeModel("gemini-1.5-flash")
                            r = await m.generate_content_async(f"Enhance video prompt (English): {prompt}")
                            enhanced_prompt = self.clean_response(r.text)
                        except: pass # ูุดู ุงููู

            # 3. ุฅุฑุณุงู ุงูุทูุจ ูุฎุฏูุฉ Luma Dream Machine
            url = "https://api.lumalabs.ai/dream-machine/v1/generations"
            headers = {
                "Authorization": f"Bearer {self.luma_api_key}",
                "Content-Type": "application/json"
            }
            payload = {
                "prompt": enhanced_prompt,
                "aspect_ratio": "16:9"
            }
            
            # ุฅุฐุง ูุงู ุงูููุฏูู ูู ุตูุฑุฉ
            if image_url:
                url = "https://api.lumalabs.ai/dream-machine/v1/generations/image"
                payload["image_url"] = image_url
            
            async with aiohttp.ClientSession() as session:
                # ุฅุฑุณุงู ุทูุจ ุงูุฅูุดุงุก
                async with session.post(url, headers=headers, json=payload) as response:
                    if response.status != 201 and response.status != 200:
                        err_text = await response.text()
                        return None, f"โ ุฎุทุฃ ูู Luma: {response.status} - {err_text[:50]}"
                    
                    data = await response.json()
                    gen_id = data.get("id")
                    if not gen_id:
                        return None, "โ ูู ูุชู ุงุณุชูุงู ูุนุฑู ุงูููุฏูู."
                    
                    # 4. ุงูุชุธุงุฑ ุงููุชูุฌุฉ (Polling)
                    # ููุชุธุฑ ุจุญุฏ ุฃูุตู 60 ูุญุงููุฉ * 5 ุซูุงูู = 5 ุฏูุงุฆู
                    for _ in range(60):
                        await asyncio.sleep(5)
                        async with session.get(f"{url}/{gen_id}", headers=headers) as check_resp:
                            if check_resp.status == 200:
                                status_data = await check_resp.json()
                                state = status_data.get("state")
                                
                                if state == "completed":
                                    video_url = status_data.get("assets", {}).get("video")
                                    if video_url:
                                        self.update_user_usage(user_id, "video_gen")
                                        self.db.save_generated_file(user_id, "video", prompt, video_url)
                                        return video_url, "โ ุชู ุฅูุดุงุก ุงูููุฏูู ุจูุฌุงุญ!"
                                elif state == "failed":
                                    return None, f"โ ูุดู ุชูููุฏ ุงูููุฏูู: {status_data.get('failure_reason')}"
            
            return None, "โ๏ธ ุงุณุชุบุฑู ุงูููุฏูู ููุชุงู ุทูููุงู ุฌุฏุงูุ ุณูุชู ุฅุดุนุงุฑู ุนูุฏ ุงูุชูุงูู."

        except Exception as e:
            logger.error(f"โ Video Error: {e}")
            return None, "ุญุฏุซ ุฎุทุฃ ุชููู ูู ุฎุฏูุฉ ุงูููุฏูู."

    # ==================== ุฏูุงู ูุณุงุนุฏุฉ ููุนูููุงุช (Utility Functions) ====================
    def get_available_services(self) -> Dict[str, bool]:
        """
        ุฅุฑุฌุงุน ุญุงูุฉ ุงูุฎุฏูุงุช ุงููุชุงุญุฉ ุญุงููุงู ุจูุงุกู ุนูู ุงูููุงุชูุญ.
        """
        return {
            "chat": self.gemini_available or self.openai_available,
            "image_generation": self.openai_available or bool(self.stability_api_key),
            "video_generation": self.luma_available
        }
        
    def get_user_stats(self, user_id: int) -> Dict[str, int]:
        """
        ุฅุฑุฌุงุน ุฅุญุตุงุฆูุงุช ุงุณุชุฎุฏุงู ุงููุณุชุฎุฏู ููููู ุงูุญุงูู.
        """
        stats = {}
        for s_type in ["ai_chat", "image_gen", "video_gen"]:
            today = datetime.now().strftime('%Y-%m-%d')
            cache_key = f"{user_id}_{today}_{s_type}"
            # ุงูุจุญุซ ูู ุงููุงุด ุฃููุงู
            stats[s_type] = self.user_limits_cache.get(cache_key, 0)
        return stats
